2025-03-29 20:57:01 [INFO] üöÄ === –ó–∞–ø—É—Å–∫ —Ç—Ä–µ–Ω–∏—Ä–æ–≤–∫–∏ (train/dev/test) ===
2025-03-29 20:57:01 [INFO] === CONFIGURATION ===
2025-03-29 20:57:01 [INFO] Split: train
2025-03-29 20:57:01 [INFO] Base Dir: E:\Databases\MELD_new\
2025-03-29 20:57:01 [INFO] CSV Path: {base_dir}\meld_{split}_labels.csv
2025-03-29 20:57:01 [INFO] WAV Dir: {base_dir}\wavs\{split}
2025-03-29 20:57:01 [INFO] Emotion columns: ['neutral', 'happy', 'sad', 'anger', 'surprise', 'disgust', 'fear']
2025-03-29 20:57:01 [INFO] --- Training Config ---
2025-03-29 20:57:01 [INFO] Sample Rate=16000, Wav Length=4s
2025-03-29 20:57:01 [INFO] Whisper Model=base, Device=cuda, MaxTokens=15
2025-03-29 20:57:01 [INFO] use_whisper_for_nontrain_if_no_text=True
2025-03-29 20:57:01 [INFO] DataLoader: batch_size=16, num_workers=0, shuffle=True
2025-03-29 20:57:01 [INFO] Model Name: MultiModalTransformer_v3
2025-03-29 20:57:01 [INFO] Random Seed: 42
2025-03-29 20:57:01 [INFO] Hidden Dim: 256
2025-03-29 20:57:01 [INFO] Hidden Dim in Gated: 256
2025-03-29 20:57:01 [INFO] Num Heads in Transformer: 4
2025-03-29 20:57:01 [INFO] Num Heads in Graph: 8
2025-03-29 20:57:01 [INFO] Mode stat pooling: mean
2025-03-29 20:57:01 [INFO] Positional Encoding: False
2025-03-29 20:57:01 [INFO] Number of transformer layers: 1
2025-03-29 20:57:01 [INFO] Dropout: 0
2025-03-29 20:57:01 [INFO] Out Features: 128
2025-03-29 20:57:01 [INFO] LR: 0.0001
2025-03-29 20:57:01 [INFO] Num Epochs: 1
2025-03-29 20:57:01 [INFO] Merge Probability=0
2025-03-29 20:57:01 [INFO] --- Embeddings Config ---
2025-03-29 20:57:01 [INFO] Audio Model: amiriparian/ExHuBERT, Text Model: jinaai/jina-embeddings-v3
2025-03-29 20:57:01 [INFO] Audio dim=1024, Text dim=1024
2025-03-29 20:57:01 [INFO] Audio pooling=None, Text pooling=None
2025-03-29 20:57:01 [INFO] Max tokens=44, Max audio frames=64000
2025-03-29 20:57:01 [INFO] Emb device=cuda, Normalize=True
2025-03-29 20:57:01 [INFO] üîí –§–∏–∫—Å–∏—Ä—É–µ–º random seed: 42
2025-03-29 20:57:04 [INFO] üìä –ê–Ω–∞–ª–∏–∑ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è —Ñ–∞–π–ª–æ–≤ –ø–æ —ç–º–æ—Ü–∏—è–º:
2025-03-29 20:57:04 [INFO] üé≠ –≠–º–æ—Ü–∏—è 'surprise': 1205 —Ñ–∞–π–ª–æ–≤.
2025-03-29 20:57:04 [INFO] üé≠ –≠–º–æ—Ü–∏—è 'neutral': 4709 —Ñ–∞–π–ª–æ–≤.
2025-03-29 20:57:04 [INFO] üé≠ –≠–º–æ—Ü–∏—è 'sad': 683 —Ñ–∞–π–ª–æ–≤.
2025-03-29 20:57:04 [INFO] üé≠ –≠–º–æ—Ü–∏—è 'fear': 268 —Ñ–∞–π–ª–æ–≤.
2025-03-29 20:57:04 [INFO] üé≠ –≠–º–æ—Ü–∏—è 'happy': 1743 —Ñ–∞–π–ª–æ–≤.
2025-03-29 20:57:04 [INFO] üé≠ –≠–º–æ—Ü–∏—è 'anger': 1109 —Ñ–∞–π–ª–æ–≤.
2025-03-29 20:57:04 [INFO] üé≠ –≠–º–æ—Ü–∏—è 'disgust': 271 —Ñ–∞–π–ª–æ–≤.
2025-03-29 20:57:04 [INFO] [DatasetMultiModal] –°–ø–ª–∏—Ç=train, –≤—Å–µ–≥–æ —Å—Ç—Ä–æ–∫: 9988
2025-03-29 20:57:05 [INFO] üîó –í—Å–µ–≥–æ —Ñ–∞–π–ª–æ–≤: 9988, –Ω—É–∂–Ω–æ —Å–∫–ª–µ–∏—Ç—å: 0 (0%)
2025-03-29 20:57:05 [INFO] üîó –ö–æ—Ä–æ—Ç–∫–∏—Ö —Ñ–∞–π–ª–æ–≤: 7563, –≤—ã–±—Ä–∞–Ω–æ –¥–ª—è —Å–∫–ª–µ–π–∫–∏: 0
2025-03-29 20:57:05 [INFO] –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è Whisper: –º–æ–¥–µ–ª—å=base, —É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ=cuda
2025-03-29 20:57:06 [INFO] üìä –ê–Ω–∞–ª–∏–∑ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è —Ñ–∞–π–ª–æ–≤ –ø–æ —ç–º–æ—Ü–∏—è–º:
2025-03-29 20:57:06 [INFO] üé≠ –≠–º–æ—Ü–∏—è 'surprise': 150 —Ñ–∞–π–ª–æ–≤.
2025-03-29 20:57:06 [INFO] üé≠ –≠–º–æ—Ü–∏—è 'sad': 111 —Ñ–∞–π–ª–æ–≤.
2025-03-29 20:57:06 [INFO] üé≠ –≠–º–æ—Ü–∏—è 'neutral': 469 —Ñ–∞–π–ª–æ–≤.
2025-03-29 20:57:06 [INFO] üé≠ –≠–º–æ—Ü–∏—è 'fear': 40 —Ñ–∞–π–ª–æ–≤.
2025-03-29 20:57:06 [INFO] üé≠ –≠–º–æ—Ü–∏—è 'happy': 163 —Ñ–∞–π–ª–æ–≤.
2025-03-29 20:57:06 [INFO] üé≠ –≠–º–æ—Ü–∏—è 'anger': 153 —Ñ–∞–π–ª–æ–≤.
2025-03-29 20:57:06 [INFO] üé≠ –≠–º–æ—Ü–∏—è 'disgust': 22 —Ñ–∞–π–ª–æ–≤.
2025-03-29 20:57:06 [INFO] [DatasetMultiModal] –°–ø–ª–∏—Ç=dev, –≤—Å–µ–≥–æ —Å—Ç—Ä–æ–∫: 1108
2025-03-29 20:57:07 [INFO] üîó –í—Å–µ–≥–æ —Ñ–∞–π–ª–æ–≤: 1108, –Ω—É–∂–Ω–æ —Å–∫–ª–µ–∏—Ç—å: 0 (0%)
2025-03-29 20:57:07 [INFO] üîó –ö–æ—Ä–æ—Ç–∫–∏—Ö —Ñ–∞–π–ª–æ–≤: 840, –≤—ã–±—Ä–∞–Ω–æ –¥–ª—è —Å–∫–ª–µ–π–∫–∏: 0
2025-03-29 20:57:07 [INFO] –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è Whisper: –º–æ–¥–µ–ª—å=base, —É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ=cuda
2025-03-29 20:57:08 [INFO] üìä –ê–Ω–∞–ª–∏–∑ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è —Ñ–∞–π–ª–æ–≤ –ø–æ —ç–º–æ—Ü–∏—è–º:
2025-03-29 20:57:08 [INFO] üé≠ –≠–º–æ—Ü–∏—è 'surprise': 281 —Ñ–∞–π–ª–æ–≤.
2025-03-29 20:57:08 [INFO] üé≠ –≠–º–æ—Ü–∏—è 'neutral': 1256 —Ñ–∞–π–ª–æ–≤.
2025-03-29 20:57:08 [INFO] üé≠ –≠–º–æ—Ü–∏—è 'sad': 208 —Ñ–∞–π–ª–æ–≤.
2025-03-29 20:57:08 [INFO] üé≠ –≠–º–æ—Ü–∏—è 'fear': 50 —Ñ–∞–π–ª–æ–≤.
2025-03-29 20:57:08 [INFO] üé≠ –≠–º–æ—Ü–∏—è 'happy': 402 —Ñ–∞–π–ª–æ–≤.
2025-03-29 20:57:08 [INFO] üé≠ –≠–º–æ—Ü–∏—è 'anger': 345 —Ñ–∞–π–ª–æ–≤.
2025-03-29 20:57:08 [INFO] üé≠ –≠–º–æ—Ü–∏—è 'disgust': 68 —Ñ–∞–π–ª–æ–≤.
2025-03-29 20:57:08 [INFO] [DatasetMultiModal] –°–ø–ª–∏—Ç=test, –≤—Å–µ–≥–æ —Å—Ç—Ä–æ–∫: 2610
2025-03-29 20:57:09 [INFO] üîó –í—Å–µ–≥–æ —Ñ–∞–π–ª–æ–≤: 2610, –Ω—É–∂–Ω–æ —Å–∫–ª–µ–∏—Ç—å: 0 (0%)
2025-03-29 20:57:09 [INFO] üîó –ö–æ—Ä–æ—Ç–∫–∏—Ö —Ñ–∞–π–ª–æ–≤: 1917, –≤—ã–±—Ä–∞–Ω–æ –¥–ª—è —Å–∫–ª–µ–π–∫–∏: 0
2025-03-29 20:57:09 [INFO] –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è Whisper: –º–æ–¥–µ–ª—å=base, —É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ=cuda
2025-03-29 20:57:10 [WARNING] [Audio] No built-in FeatureExtractor found. Model=amiriparian/ExHuBERT. Error: amiriparian/ExHuBERT does not appear to have a file named preprocessor_config.json. Checkout 'https://huggingface.co/amiriparian/ExHuBERT/tree/main' for available files.
2025-03-29 20:57:12 [INFO] [Audio] Loaded AutoModel with output_hidden_states=True: amiriparian/ExHuBERT
2025-03-29 20:57:12 [INFO] [Text] Loading tokenizer for jinaai/jina-embeddings-v3 with trust_remote_code=True
2025-03-29 20:57:13 [INFO] [Text] Loading model for jinaai/jina-embeddings-v3 with trust_remote_code=True
2025-03-29 20:57:14 [WARNING] flash_attn is not installed. Using PyTorch native attention implementation.
2025-03-29 20:57:14 [WARNING] flash_attn is not installed. Using PyTorch native attention implementation.
2025-03-29 20:57:14 [WARNING] flash_attn is not installed. Using PyTorch native attention implementation.
2025-03-29 20:57:14 [WARNING] flash_attn is not installed. Using PyTorch native attention implementation.
2025-03-29 20:57:14 [WARNING] flash_attn is not installed. Using PyTorch native attention implementation.
2025-03-29 20:57:14 [WARNING] flash_attn is not installed. Using PyTorch native attention implementation.
2025-03-29 20:57:14 [WARNING] flash_attn is not installed. Using PyTorch native attention implementation.
2025-03-29 20:57:14 [WARNING] flash_attn is not installed. Using PyTorch native attention implementation.
2025-03-29 20:57:14 [WARNING] flash_attn is not installed. Using PyTorch native attention implementation.
2025-03-29 20:57:14 [WARNING] flash_attn is not installed. Using PyTorch native attention implementation.
2025-03-29 20:57:14 [WARNING] flash_attn is not installed. Using PyTorch native attention implementation.
2025-03-29 20:57:14 [WARNING] flash_attn is not installed. Using PyTorch native attention implementation.
2025-03-29 20:57:14 [WARNING] flash_attn is not installed. Using PyTorch native attention implementation.
2025-03-29 20:57:14 [WARNING] flash_attn is not installed. Using PyTorch native attention implementation.
2025-03-29 20:57:14 [WARNING] flash_attn is not installed. Using PyTorch native attention implementation.
2025-03-29 20:57:14 [WARNING] flash_attn is not installed. Using PyTorch native attention implementation.
2025-03-29 20:57:14 [WARNING] flash_attn is not installed. Using PyTorch native attention implementation.
2025-03-29 20:57:14 [WARNING] flash_attn is not installed. Using PyTorch native attention implementation.
2025-03-29 20:57:14 [WARNING] flash_attn is not installed. Using PyTorch native attention implementation.
2025-03-29 20:57:14 [WARNING] flash_attn is not installed. Using PyTorch native attention implementation.
2025-03-29 20:57:14 [WARNING] flash_attn is not installed. Using PyTorch native attention implementation.
2025-03-29 20:57:14 [WARNING] flash_attn is not installed. Using PyTorch native attention implementation.
2025-03-29 20:57:14 [WARNING] flash_attn is not installed. Using PyTorch native attention implementation.
2025-03-29 20:57:14 [WARNING] flash_attn is not installed. Using PyTorch native attention implementation.
2025-03-29 20:57:14 [WARNING] flash_attn is not installed. Using PyTorch native attention implementation.
2025-03-29 20:57:15 [INFO] 
=== –≠–ø–æ—Ö–∞ 0 ===
2025-03-29 21:06:06 [INFO] [TRAIN] Loss=0.6748, UAR=0.5408, WAR=0.7714, MF1=0.5312, WF1=0.7509, MEAN=0.6486
2025-03-29 21:06:28 [INFO] [DEV]   Loss=1.9458, UAR=0.2856, WAR=0.4801, MF1=0.2855, WF1=0.4530, MEAN=0.3761
2025-03-29 21:07:19 [INFO] [TEST]  Loss=1.9316, UAR=0.2619, WAR=0.4828, MF1=0.2613, WF1=0.4576, MEAN=0.3659
2025-03-29 21:07:19 [INFO] ‚úÖ –¢—Ä–µ–Ω–∏—Ä–æ–≤–∫–∞ –∑–∞–≤–µ—Ä—à–µ–Ω–∞. –í—Å–µ split'—ã –æ–±—Ä–∞–±–æ—Ç–∞–Ω—ã!
