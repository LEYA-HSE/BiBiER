{
 "cells": [
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Some weights of the model checkpoint at audeering/wav2vec2-large-robust-12-ft-emotion-msp-dim were not used when initializing EmotionModel: ['classifier.dense.weight', 'classifier.out_proj.weight', 'classifier.dense.bias', 'classifier.out_proj.bias']\n",
      "- This IS expected if you are initializing EmotionModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing EmotionModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Найдено 5 аудио файлов.\n",
      "Обработан файл: 09_neutral_happiness n_060.wav (id=09_neutral_happiness n_060.wav), эмбеддингов: (1, 538, 1024), предсказанная эмоция: fear\n",
      "Обработан файл: 09_neutral_happiness n_080.wav (id=09_neutral_happiness n_080.wav), эмбеддингов: (1, 651, 1024), предсказанная эмоция: neutral\n",
      "Обработан файл: 09_neutral_happiness n_052.wav (id=09_neutral_happiness n_052.wav), эмбеддингов: (1, 527, 1024), предсказанная эмоция: neutral\n",
      "Обработан файл: 09_neutral_happiness h_071.wav (id=09_neutral_happiness h_071.wav), эмбеддингов: (1, 633, 1024), предсказанная эмоция: joy/happiness\n",
      "Обработан файл: 09_neutral_happiness n_020.wav (id=09_neutral_happiness n_020.wav), эмбеддингов: (1, 275, 1024), предсказанная эмоция: surprise/enthusiasm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "import torch\n",
    "import librosa\n",
    "import torch.nn as nn\n",
    "from transformers import Wav2Vec2Processor\n",
    "from transformers.models.wav2vec2.modeling_wav2vec2 import (\n",
    "    Wav2Vec2Model,\n",
    "    Wav2Vec2PreTrainedModel,\n",
    ")\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "class CustomMambaBlock(nn.Module):\n",
    "    def __init__(self, d_input, d_model, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.in_proj = nn.Linear(d_input, d_model)\n",
    "        self.s_B = nn.Linear(d_model, d_model)\n",
    "        self.s_C = nn.Linear(d_model, d_model)\n",
    "        self.out_proj = nn.Linear(d_model, d_input)\n",
    "        self.norm = nn.LayerNorm(d_input)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.activation = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x_in = x  # сохраняем вход\n",
    "        x = self.in_proj(x)\n",
    "        B = self.s_B(x)\n",
    "        C = self.s_C(x)\n",
    "        x = x + B + C\n",
    "        x = self.activation(x)\n",
    "        x = self.out_proj(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.norm(x + x_in)  # residual + norm\n",
    "        return x\n",
    "\n",
    "class CustomMambaClassifier(nn.Module):\n",
    "    def __init__(self, input_size=1024, d_model=256, num_layers=2, num_classes=7, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.input_proj = nn.Linear(input_size, d_model)\n",
    "        self.blocks = nn.ModuleList([\n",
    "            CustomMambaBlock(d_model, d_model, dropout=dropout)\n",
    "            for _ in range(num_layers)\n",
    "        ])\n",
    "        self.fc = nn.Linear(d_model, num_classes)\n",
    "\n",
    "    def forward(self, x, lengths):\n",
    "        # x: (batch, seq_length, input_size)\n",
    "        x = self.input_proj(x)\n",
    "        for block in self.blocks:\n",
    "            x = block(x)\n",
    "        pooled = []\n",
    "        for i, l in enumerate(lengths):\n",
    "            if l > 0:\n",
    "                pooled.append(x[i, :l, :].mean(dim=0))\n",
    "            else:\n",
    "                pooled.append(torch.zeros(x.size(2), device=x.device))\n",
    "        pooled = torch.stack(pooled, dim=0)\n",
    "        return self.fc(pooled)\n",
    "\n",
    "def get_model_mamba(params):\n",
    "    return CustomMambaClassifier(\n",
    "        input_size=params.get(\"input_size\", 1024),\n",
    "        d_model=params.get(\"d_model\", 256),\n",
    "        num_layers=params.get(\"num_layers\", 2),\n",
    "        num_classes=params.get(\"num_classes\", 7),\n",
    "        dropout=params.get(\"dropout\", 0.1)\n",
    "    )\n",
    "\n",
    "label_to_emotion = {\n",
    "    0: 'anger',\n",
    "    1: 'disgust',\n",
    "    2: 'fear',\n",
    "    3: 'joy/happiness',\n",
    "    4: 'neutral',\n",
    "    5: 'sadness',\n",
    "    6: 'surprise/enthusiasm'\n",
    "}\n",
    "\n",
    "class EmotionModel(Wav2Vec2PreTrainedModel):\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "        self.config = config\n",
    "        self.wav2vec2 = Wav2Vec2Model(config)\n",
    "        self.init_weights()\n",
    "\n",
    "    def forward(self, input_values):\n",
    "        outputs = self.wav2vec2(input_values)\n",
    "        hidden_states = outputs[0]  # (batch_size, sequence_length, hidden_size)\n",
    "        return hidden_states\n",
    "\n",
    "\n",
    "model_name = 'audeering/wav2vec2-large-robust-12-ft-emotion-msp-dim'\n",
    "processor = Wav2Vec2Processor.from_pretrained(model_name)\n",
    "audio_embedder = EmotionModel.from_pretrained(model_name).to(device)\n",
    "\n",
    "model_params = {\n",
    "        \"input_size\": 1024,\n",
    "        \"d_model\": 256,\n",
    "        \"num_layers\": 2,\n",
    "        \"num_classes\": 7,\n",
    "        \"dropout\": 0.2\n",
    "    }\n",
    "\n",
    "\n",
    "def process_audio(signal: np.ndarray, sampling_rate: int) -> np.ndarray:\n",
    "    inputs = processor(signal, sampling_rate=sampling_rate, return_tensors=\"pt\", padding=True)\n",
    "    input_values = inputs[\"input_values\"].to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = audio_embedder(input_values)\n",
    "        embeddings = outputs\n",
    "        \n",
    "    return embeddings.detach().cpu().numpy()\n",
    "\n",
    "def load_classifier_model_from_checkpoint(checkpoint_path):\n",
    "    classifier_model = get_model_mamba(model_params).to(device)\n",
    "    classifier_model.load_state_dict(torch.load(checkpoint_path, map_location=device))\n",
    "    classifier_model.eval()\n",
    "    return classifier_model\n",
    "\n",
    "classifier_checkpoint = \"new_best_model.pt\"\n",
    "classifier_model = load_classifier_model_from_checkpoint(classifier_checkpoint)\n",
    "\n",
    "audio_dir = \"RESD_RAW/test/09_neutral_happiness\" \n",
    "\n",
    "embedding_dict = {}\n",
    "\n",
    "audio_files = [f for f in os.listdir(audio_dir) if f.lower().endswith('.wav')]\n",
    "\n",
    "print(f\"Найдено {len(audio_files)} аудио файлов.\\n\")\n",
    "\n",
    "for file_name in audio_files:\n",
    "    \n",
    "    file_id = file_name\n",
    "    file_path = os.path.join(audio_dir, file_name)\n",
    "    \n",
    "    signal, sr = librosa.load(file_path, sr=16000)\n",
    "\n",
    "    embeddings = process_audio(signal, sr)\n",
    "\n",
    "    tensor_emb = torch.tensor(embeddings, dtype=torch.float32).to(device)\n",
    "    lengths = [tensor_emb.shape[1]] \n",
    "\n",
    "    with torch.no_grad():\n",
    "        logits = classifier_model(tensor_emb, lengths)\n",
    "        pred_idx = torch.argmax(logits, dim=1).item()\n",
    "        predicted_emotion = label_to_emotion.get(pred_idx, \"Unknown\")\n",
    "    \n",
    "    embedding_dict[file_id] = {\n",
    "        \"embeddings\": embeddings, \n",
    "        \"predicted_emotion\": predicted_emotion\n",
    "    }\n",
    "    \n",
    "    print(f\"Обработан файл: {file_name} (id={file_id}), эмбеддингов: {embeddings.shape}, предсказанная эмоция: {predicted_emotion}\")\n",
    "    "
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-04-14T19:56:26.154874Z",
     "start_time": "2025-04-14T19:56:10.847055Z"
    }
   },
   "id": "c112be31762c1206",
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "b846c2f7debf0feb"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
