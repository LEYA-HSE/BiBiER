# -*- coding: utf-8 -*-

import os
import random
import logging
import torch
import torchaudio
import whisper
import numpy as np
import pandas as pd
from torch.utils.data import Dataset
import pickle
from tqdm import tqdm
# from data_loading.feature_extractor import PretrainedAudioEmbeddingExtractor, PretrainedTextEmbeddingExtractor

class DatasetMultiModalWithPretrainedExtractors(Dataset):
    """
    –ú—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã–π –¥–∞—Ç–∞—Å–µ—Ç –¥–ª—è –∞—É–¥–∏–æ, —Ç–µ–∫—Å—Ç–∞ –∏ —ç–º–æ—Ü–∏–π (–æ–Ω‚Äëthe‚Äëfly –≤–µ—Ä—Å–∏—è).

    –ü—Ä–∏ –∫–∞–∂–¥–æ–º –≤—ã–∑–æ–≤–µ __getitem__:
      - –ó–∞–≥—Ä—É–∂–∞–µ—Ç WAV –ø–æ video_name –∏–∑ CSV.
      - –î–ª—è –æ–±—É—á–∞—é—â–µ–π –≤—ã–±–æ—Ä–∫–∏ (split="train"):
            –ï—Å–ª–∏ –∞—É–¥–∏–æ –∫–æ—Ä–æ—á–µ target_samples, –ø—Ä–æ–≤–µ—Ä—è–µ–º, –≤—ã–±—Ä–∞–ª–∏ –ª–∏ –º—ã —ç—Ç–æ—Ç —Ñ–∞–π–ª –¥–ª—è —Å–∫–ª–µ–π–∫–∏
            (–ø–æ merge_probability). –ï—Å–ª–∏ –¥–∞ ‚Äì –≤—ã–ø–æ–ª–Ω—è–µ—Ç—Å—è "chain merge":
            –≤—ã–±–∏—Ä–∞–µ—Ç—Å—è –æ–¥–∏–Ω –∏–ª–∏ –Ω–µ—Å–∫–æ–ª—å–∫–æ –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã—Ö —Ñ–∞–π–ª–æ–≤ —Ç–æ–≥–æ –∂–µ –∫–ª–∞—Å—Å–∞, –¥–∞–∂–µ –µ—Å–ª–∏ –æ–¥–∏–Ω –∫–∞–Ω–¥–∏–¥–∞—Ç –¥–ª–∏–Ω–Ω–µ–µ,
            –∏ –∏—Ç–æ–≥–æ–≤–æ–µ –∞—É–¥–∏–æ –∑–∞—Ç–µ–º –æ–±—Ä–µ–∑–∞–µ—Ç—Å—è –¥–æ —Ç–æ—á–Ω–æ–π –¥–ª–∏–Ω—ã.
      - –ï—Å–ª–∏ –∏—Ç–æ–≥–æ–≤–æ–µ –∞—É–¥–∏–æ –≤—Å—ë –µ—â—ë –º–µ–Ω—å—à–µ target_samples, –≤—ã–ø–æ–ª–Ω—è–µ—Ç—Å—è –ø–∞–¥–¥–∏–Ω–≥ –Ω—É–ª—è–º–∏.
      - –¢–µ–∫—Å—Ç –≤—ã–±–∏—Ä–∞–µ—Ç—Å—è —Ç–∞–∫:
            ‚Ä¢ –ï—Å–ª–∏ –∞—É–¥–∏–æ –±—ã–ª–æ merged (—Å–∫–ª–µ–µ–Ω–æ) ‚Äì –≤—ã–∑—ã–≤–∞–µ—Ç—Å—è Whisper –¥–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è –Ω–æ–≤–æ–≥–æ —Ç–µ–∫—Å—Ç–∞.
            ‚Ä¢ –ï—Å–ª–∏ merge –Ω–µ –ø—Ä–æ–∏—Å—Ö–æ–¥–∏–ª–æ –∏ CSV-—Ç–µ–∫—Å—Ç –Ω–µ –ø—É—Å—Ç ‚Äì –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è CSV-—Ç–µ–∫—Å—Ç.
            ‚Ä¢ –ï—Å–ª–∏ CSV-—Ç–µ–∫—Å—Ç –ø—É—Å—Ç–æ–π ‚Äì –¥–ª—è train (–∏–ª–∏, –ø—Ä–∏ —É—Å–ª–æ–≤–∏–∏, –¥–ª—è dev/test) –≤—ã–∑—ã–≤–∞–µ—Ç—Å—è Whisper.
      - –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å–ª–æ–≤–∞—Ä—å { "audio": waveform, "label": label_vector, "text": text_final }.
    """

    def __init__(
        self,
        csv_path,
        wav_dir,
        emotion_columns,
        config,
        split,
        audio_feature_extractor,
        text_feature_extractor,
        whisper_model,
        dataset_name
    ):
        """
        :param csv_path: –ü—É—Ç—å –∫ CSV-—Ñ–∞–π–ª—É (—Å –∫–æ–ª–æ–Ω–∫–∞–º–∏ video_name, emotion_columns, –≤–æ–∑–º–æ–∂–Ω–æ text).
        :param wav_dir: –ü–∞–ø–∫–∞ —Å –∞—É–¥–∏–æ—Ñ–∞–π–ª–∞–º–∏ (–∏–º—è —Ñ–∞–π–ª–∞: video_name.wav).
        :param emotion_columns: –°–ø–∏—Å–æ–∫ –∫–æ–ª–æ–Ω–æ–∫ —ç–º–æ—Ü–∏–π, –Ω–∞–ø—Ä–∏–º–µ—Ä ["neutral", "happy", "sad", ...].
        :param split: "train", "dev" –∏–ª–∏ "test".
        :param audio_feature_extractor: –≠–∫—Å—Ç—Ä–∞–∫—Ç–æ—Ä –∞—É–¥–∏–æ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
        :param text_feature_extractor: –≠–∫—Å—Ç—Ä–∞–∫—Ç–æ—Ä —Ç–µ–∫—Å—Ç–æ–≤—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
        :param sample_rate: –¶–µ–ª–µ–≤–∞—è —á–∞—Å—Ç–æ—Ç–∞ –¥–∏—Å–∫—Ä–µ—Ç–∏–∑–∞—Ü–∏–∏ (–Ω–∞–ø—Ä–∏–º–µ—Ä, 16000).
        :param wav_length: –¶–µ–ª–µ–≤–∞—è –¥–ª–∏–Ω–∞ –∞—É–¥–∏–æ –≤ —Å–µ–∫—É–Ω–¥–∞—Ö.
        :param whisper_model: M–æ–¥–µ–ª—å Whisper ("tiny", "base", "small", ...).
        :param max_text_tokens: (–ù–µ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è) ‚Äì –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–µ –Ω–∞ —á–∏—Å–ª–æ —Ç–æ–∫–µ–Ω–æ–≤.
        :param text_column: –ù–∞–∑–≤–∞–Ω–∏–µ –∫–æ–ª–æ–Ω–∫–∏ —Å —Ç–µ–∫—Å—Ç–æ–º –≤ CSV.
        :param use_whisper_for_nontrain_if_no_text: –ï—Å–ª–∏ True, –¥–ª—è dev/test –ø—Ä–∏ –æ—Ç—Å—É—Ç—Å—Ç–≤–∏–∏ CSV-—Ç–µ–∫—Å—Ç–∞ –≤—ã–∑—ã–≤–∞–µ—Ç—Å—è Whisper.
        :param whisper_device: "cuda" –∏–ª–∏ "cpu" ‚Äì —É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ –¥–ª—è –º–æ–¥–µ–ª–∏ Whisper.
        :param subset_size: –ï—Å–ª–∏ > 0, –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è —Ç–æ–ª—å–∫–æ –ø–µ—Ä–≤—ã–µ N –∑–∞–ø–∏—Å–µ–π –∏–∑ CSV (–¥–ª—è –æ—Ç–ª–∞–¥–∫–∏).
        :param merge_probability: –ü—Ä–æ—Ü–µ–Ω—Ç (0..1) –æ—Ç –≤—Å–µ–≥–æ —á–∏—Å–ª–∞ —Ñ–∞–π–ª–æ–≤, –∫–æ—Ç–æ—Ä—ã–µ –±—É–¥—É—Ç —Å–∫–ª–µ–∏–≤–∞—Ç—å—Å—è, –µ—Å–ª–∏ –æ–Ω–∏ –∫–æ—Ä–æ—á–µ.
        :param dataset_name: –ù–∞–∑–≤–∞–Ω–∏–µ –∫–æ—Ä–ø—É—Å–∞
        """
        super().__init__()
        self.split = split
        self.sample_rate = config.sample_rate
        self.target_samples = int(config.wav_length * self.sample_rate)
        self.emotion_columns = emotion_columns
        self.whisper_model = whisper_model
        self.text_column =  config.text_column
        self.use_whisper_for_nontrain_if_no_text =  config.use_whisper_for_nontrain_if_no_text
        self.whisper_device = config.whisper_device
        self.merge_probability = config.merge_probability
        self.audio_feature_extractor = audio_feature_extractor
        self.text_feature_extractor = text_feature_extractor
        self.subset_size    = config.subset_size
        self.save_prepared_data = config.save_prepared_data
        self.seed = config.random_seed
        self.dataset_name = dataset_name
        self.save_feature_path = config.save_feature_path
        self.use_synthetic_data = config.use_synthetic_data
        self.synthetic_path = config.synthetic_path
        self.synthetic_ratio = config.synthetic_ratio

        # –ó–∞–≥—Ä—É–∂–∞–µ–º CSV
        if not os.path.exists(csv_path):
            raise ValueError(f"–û—à–∏–±–∫–∞: —Ñ–∞–π–ª CSV –Ω–µ –Ω–∞–π–¥–µ–Ω: {csv_path}")
        df = pd.read_csv(csv_path)
        if self.subset_size > 0:
            df = df.head(self.subset_size)
            logging.info(f"[DatasetMultiModal] –ò—Å–ø–æ–ª—å–∑—É–µ–º —Ç–æ–ª—å–∫–æ –ø–µ—Ä–≤—ã–µ {len(df)} –∑–∞–ø–∏—Å–µ–π (subset_size={self.subset_size}).")

        #–∫–æ–ø–∏—è –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è —Ç–µ–∫—Å—Ç–∞ Wisper
        self.original_df = df.copy()
        self.whisper_csv_update_log = []

        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ –≤—Å–µ—Ö –∫–æ–ª–æ–Ω–æ–∫ —ç–º–æ—Ü–∏–π
        missing = [c for c in emotion_columns if c not in df.columns]
        if missing:
            raise ValueError(f"–í CSV –æ—Ç—Å—É—Ç—Å—Ç–≤—É—é—Ç –Ω–µ–æ–±—Ö–æ–¥–∏–º—ã–µ –∫–æ–ª–æ–Ω–∫–∏ —ç–º–æ—Ü–∏–π: {missing}")

        # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å—É—â–µ—Å—Ç–≤–æ–≤–∞–Ω–∏–µ –ø–∞–ø–∫–∏ —Å –∞—É–¥–∏–æ
        if not os.path.exists(wav_dir):
            raise ValueError(f"–û—à–∏–±–∫–∞: –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—è —Å –∞—É–¥–∏–æ {wav_dir} –Ω–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç!")
        self.wav_dir = wav_dir

        # –°–æ–±–∏—Ä–∞–µ–º —Å–ø–∏—Å–æ–∫ —Å—Ç—Ä–æ–∫: –¥–ª—è –∫–∞–∂–¥–æ–π –∑–∞–ø–∏—Å–∏ –ø–æ–ª—É—á–∞–µ–º –ø—É—Ç—å –∫ –∞—É–¥–∏–æ, label –∏ CSV-—Ç–µ–∫—Å—Ç (–µ—Å–ª–∏ –µ—Å—Ç—å)
        self.rows = []
        for i, rowi in df.iterrows():
            audio_path = os.path.join(wav_dir, f"{rowi['video_name']}.wav")
            if not os.path.exists(audio_path):
                continue
            # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –¥–æ–º–∏–Ω–∏—Ä—É—é—â—É—é —ç–º–æ—Ü–∏—é (–º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∑–Ω–∞—á–µ–Ω–∏–µ)
            # print(self.emotion_columns)
            emotion_values = rowi[self.emotion_columns].values.astype(float)
            max_idx = np.argmax(emotion_values)
            emotion_label = self.emotion_columns[max_idx]

            # –ò–∑–≤–ª–µ–∫–∞–µ–º —Ç–µ–∫—Å—Ç –∏–∑ CSV (–µ—Å–ª–∏ –µ—Å—Ç—å)
            csv_text = ""
            if self.text_column in rowi and isinstance(rowi[self.text_column], str):
                csv_text = rowi[self.text_column]

            self.rows.append({
                "audio_path": audio_path,
                "label": emotion_label,
                "csv_text": csv_text
            })

        if self.use_synthetic_data and self.split == "train" and self.dataset_name.lower() == "meld":
            logging.info(f"üß™ –í–∫–ª—é—á–µ–Ω–∞ —Å–∏–Ω—Ç–µ—Ç–∏–∫–∞ –¥–ª—è –¥–∞—Ç–∞—Å–µ—Ç–∞ '{self.dataset_name}' ‚Äî –¥–æ–±–∞–≤–ª—è–µ–º –ø—Ä–∏–º–µ—Ä—ã –∏–∑: {self.synthetic_path}")
            self._add_synthetic_data(self.synthetic_ratio)

        # –°–æ–∑–¥–∞–µ–º –∫–∞—Ä—Ç—É –¥–ª—è –ø–æ–∏—Å–∫–∞ —Ñ–∞–π–ª–æ–≤ –ø–æ —ç–º–æ—Ü–∏–∏
        self.audio_class_map = {entry["audio_path"]: entry["label"] for entry in self.rows}

        logging.info("üìä –ê–Ω–∞–ª–∏–∑ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è —Ñ–∞–π–ª–æ–≤ –ø–æ —ç–º–æ—Ü–∏—è–º:")
        emotion_counts = {emotion: 0 for emotion in set(self.audio_class_map.values())}
        for path, emotion in self.audio_class_map.items():
            emotion_counts[emotion] += 1
        for emotion, count in emotion_counts.items():
            logging.info(f"üé≠ –≠–º–æ—Ü–∏—è '{emotion}': {count} —Ñ–∞–π–ª–æ–≤.")

        logging.info(f"[DatasetMultiModal] –°–ø–ª–∏—Ç={split}, –≤—Å–µ–≥–æ —Å—Ç—Ä–æ–∫: {len(self.rows)}")

        # === –ü—Ä–æ—Ü–µ–Ω—Ç–Ω–æ–µ —Å–µ–º–ø–ª–∏—Ä–æ–≤–∞–Ω–∏–µ ===
        total_files = len(self.rows)
        num_to_merge = int(total_files * self.merge_probability)

        # <<< NEW: –ö–µ—à–∏—Ä—É–µ–º –¥–ª–∏–Ω—ã (eq_len) –¥–ª—è –≤—Å–µ—Ö —Ñ–∞–π–ª–æ–≤ >>>
        self.path_info = {}
        for row in self.rows:
            p = row["audio_path"]
            try:
                info = torchaudio.info(p)
                length = info.num_frames
                sr_ = info.sample_rate
                # –ø–µ—Ä–µ–≤–æ–¥–∏–º –¥–ª–∏–Ω—É –≤ "—ç–∫–≤–∏–≤–∞–ª–µ–Ω—Ç self.sample_rate"
                if sr_ != self.sample_rate:
                    ratio = sr_ / self.sample_rate
                    eq_len = int(length / ratio)
                else:
                    eq_len = length
                self.path_info[p] = eq_len
            except Exception as e:
                logging.warning(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ —á—Ç–µ–Ω–∏—è {p}: {e}")
                self.path_info[p] = 0  # –ï—Å–ª–∏ –Ω–µ —Å–º–æ–≥–ª–∏ –ø—Ä–æ—á–∏—Ç–∞—Ç—å, —Å—Ç–∞–≤–∏–º 0

        # –û–ø—Ä–µ–¥–µ–ª–∏–º, –∫–∞–∫–∏–µ —Ñ–∞–π–ª—ã "–∫–æ—Ä–æ—Ç–∫–∏–µ" (–º–æ–≥—É—Ç –Ω—É–∂–¥–∞—Ç—å—Å—è –≤ —Å–∫–ª–µ–π–∫–µ) - –∏—Å–ø–æ–ª—å–∑—É–µ–º –∫—ç—à –≤–º–µ—Å—Ç–æ —Å—Ç–∞—Ä–æ–≥–æ _is_too_short
        self.mergable_files = [
            row["audio_path"]  # –≤–º–µ—Å—Ç–æ —Ü–µ–ª–æ–≥–æ dict –±–µ—Ä—ë–º —Å—Ç—Ä–æ–∫—É
            for row in self.rows
            if self._is_too_short_cached(row["audio_path"])  # <<< —Ç–µ–ø–µ—Ä—å —Ç—É—Ç –∏—Å–ø–æ–ª—å–∑—É–µ–º –Ω–æ–≤—É—é —Ñ—É–Ω–∫—Ü–∏—é
        ]
        short_count = len(self.mergable_files)

        # –ï—Å–ª–∏ –∫–æ—Ä–æ—Ç–∫–∏—Ö —Ñ–∞–π–ª–æ–≤ –±–æ–ª—å—à–µ –Ω—É–∂–Ω–æ–≥–æ —á–∏—Å–ª–∞, –≤—ã–±–µ—Ä–µ–º —Å–ª—É—á–∞–π–Ω—ã–µ. –ò–Ω–∞—á–µ –≤—Å–µ –∫–æ—Ä–æ—Ç–∫–∏–µ.
        if short_count > num_to_merge:
            self.files_to_merge = set(random.sample(self.mergable_files, num_to_merge))
        else:
            self.files_to_merge = set(self.mergable_files)

        logging.info(f"üîó –í—Å–µ–≥–æ —Ñ–∞–π–ª–æ–≤: {total_files}, –Ω—É–∂–Ω–æ —Å–∫–ª–µ–∏—Ç—å: {num_to_merge} ({self.merge_probability*100:.0f}%)")
        logging.info(f"üîó –ö–æ—Ä–æ—Ç–∫–∏—Ö —Ñ–∞–π–ª–æ–≤: {short_count}, –≤—ã–±—Ä–∞–Ω–æ –¥–ª—è —Å–∫–ª–µ–π–∫–∏: {len(self.files_to_merge)}")

        if self.save_prepared_data:
            self.meta = []

            if self.use_synthetic_data:
                meta_filename = '{}_{}_seed_{}_subset_size_{}_audio_model_{}_feature_norm_{}_synthetic_true_pct_{}_pred.pickle'.format(
                    self.dataset_name,
                    self.split,
                    config.audio_classifier_checkpoint[-4:-3],
                    self.seed,
                    self.subset_size,
                    config.emb_normalize,
                    int(self.synthetic_ratio * 100)
                )

            else:
                meta_filename = '{}_{}_seed_{}_subset_size_{}_audio_model_{}_feature_norm_{}_merge_prob_{}_pred.pickle'.format(
                    self.dataset_name,
                    self.split,
                    config.audio_classifier_checkpoint[-4:-3],
                    self.seed,
                    self.subset_size,
                    config.emb_normalize,
                    self.merge_probability
                )

            pickle_path = os.path.join(self.save_feature_path, meta_filename)
            self.load_data(pickle_path)

            if not self.meta:
                self.prepare_data()
                os.makedirs(self.save_feature_path, exist_ok=True)
                self.save_data(pickle_path)

    def save_data(self, filename):
        with open(filename, 'wb') as handle:
            pickle.dump(self.meta, handle, protocol=pickle.HIGHEST_PROTOCOL)

    def load_data(self, filename):
        if os.path.exists(filename):
            with open(filename, 'rb') as handle:
                self.meta = pickle.load(handle)
        else:
            self.meta = []

    def _is_too_short(self, audio_path):
        """
        (–û—Ä–∏–≥–∏–Ω–∞–ª—å–Ω–∞—è) –ü—Ä–æ–≤–µ—Ä—è–µ–º, —è–≤–ª—è–µ—Ç—Å—è –ª–∏ —Ñ–∞–π–ª –∫–æ—Ä–æ—á–µ target_samples.
        –ò—Å–ø–æ–ª—å–∑—É–µ—Ç torchaudio.info(audio_path).
        –ù–æ —Ç–µ–ø–µ—Ä—å —ç—Ç–æ—Ç –º–µ—Ç–æ–¥ –Ω–µ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è, –ø–æ—Å–∫–æ–ª—å–∫—É –º—ã –∫–µ—à–∏—Ä—É–µ–º –¥–ª–∏–Ω—ã.
        """
        try:
            info = torchaudio.info(audio_path)
            length = info.num_frames
            sr_ = info.sample_rate
            # –ø–µ—Ä–µ–≤–æ–¥–∏–º –¥–ª–∏–Ω—É –≤ "—ç–∫–≤–∏–≤–∞–ª–µ–Ω—Ç self.sample_rate"
            if sr_ != self.sample_rate:
                ratio = sr_ / self.sample_rate
                eq_len = int(length / ratio)
            else:
                eq_len = length
            return eq_len < self.target_samples
        except Exception as e:
            logging.warning(f"–û—à–∏–±–∫–∞ _is_too_short({audio_path}): {e}")
            return False

    def _is_too_short_cached(self, audio_path):
        """
        (–ù–æ–≤–∞—è) –ü—Ä–æ–≤–µ—Ä—è–µ–º, —è–≤–ª—è–µ—Ç—Å—è –ª–∏ —Ñ–∞–π–ª –∫–æ—Ä–æ—á–µ target_samples, –∏—Å–ø–æ–ª—å–∑—É—è –∑–∞–∫–µ—à–∏—Ä–æ–≤–∞–Ω–Ω—É—é –¥–ª–∏–Ω—É –≤ self.path_info.
        """
        eq_len = self.path_info.get(audio_path, 0)
        return eq_len < self.target_samples

    def __len__(self):
        if self.save_prepared_data:
            return len(self.meta)
        else:
            return len(self.rows)

    def get_data(self, row):
        audio_path = row["audio_path"]
        label_name = row["label"]
        csv_text = row["csv_text"]

        # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º label –≤ one-hot –≤–µ–∫—Ç–æ—Ä
        label_vec = self.emotion_to_vector(label_name)

        # –®–∞–≥ 1. –ó–∞–≥—Ä—É–∂–∞–µ–º –∞—É–¥–∏–æ
        waveform, sr = self.load_audio(audio_path)
        if waveform is None:
            return None

        orig_len = waveform.shape[1]
        logging.debug(f"–ò—Å—Ö–æ–¥–Ω–∞—è –¥–ª–∏–Ω–∞ {os.path.basename(audio_path)}: {orig_len/sr:.2f} —Å–µ–∫")

        was_merged = False
        merged_texts = [csv_text]  # –¢–µ–∫—Å—Ç—ã –∏—Å—Ö–æ–¥–Ω–æ–≥–æ —Ñ–∞–π–ª–∞ + –¥–æ–±–∞–≤–ª–µ–Ω–Ω—ã—Ö

        # –®–∞–≥ 2. –î–ª—è train, –µ—Å–ª–∏ –∞—É–¥–∏–æ –∫–æ—Ä–æ—á–µ target_samples, –ø—Ä–æ–≤–µ—Ä—è–µ–º:
        #        –ø–æ–ø–∞–ª –ª–∏ –¥–∞–Ω–Ω—ã–π row –≤ files_to_merge?
        if self.split == "train" and row["audio_path"] in self.files_to_merge:
            # chain merge
            current_length = orig_len
            used_candidates = set()

            while current_length < self.target_samples:
                needed = self.target_samples - current_length
                candidate = self.get_suitable_audio(label_name, exclude_path=audio_path, min_needed=needed, top_k=10)
                if candidate is None or candidate in used_candidates:
                    break
                used_candidates.add(candidate)
                add_wf, add_sr = self.load_audio(candidate)
                if add_wf is None:
                    break
                logging.debug(f"–°–∫–ª–µ–π–∫–∞: –¥–æ–±–∞–≤–ª—è–µ–º {os.path.basename(candidate)} (–Ω–µ–æ–±—Ö–æ–¥–∏–º—ã—Ö —Å—ç–º–ø–ª–æ–≤: {needed})")
                waveform = torch.cat((waveform, add_wf), dim=1)
                current_length = waveform.shape[1]
                was_merged = True

                # –ü–æ–ª—É—á–∞–µ–º —Ç–µ–∫—Å—Ç –≤—Ç–æ—Ä–æ–≥–æ —Ñ–∞–π–ª–∞ (–µ—Å–ª–∏ –µ—Å—Ç—å –≤ CSV)
                add_csv_text = next((r["csv_text"] for r in self.rows if r["audio_path"] == candidate), "")
                merged_texts.append(add_csv_text)

                logging.debug(f"üìú –¢–µ–∫—Å—Ç –ø–µ—Ä–≤–æ–≥–æ —Ñ–∞–π–ª–∞: {csv_text}")
                logging.debug(f"üìú –¢–µ–∫—Å—Ç –¥–æ–±–∞–≤–ª–µ–Ω–Ω–æ–≥–æ —Ñ–∞–π–ª–∞: {add_csv_text}")
        else:
            # –ï—Å–ª–∏ —Ñ–∞–π–ª –Ω–µ –≤ —Å–ø–∏—Å–∫–µ "–¥–æ–ª–∂–Ω—ã —Å–∫–ª–µ–∏—Ç—å" –∏–ª–∏ —Å–ø–ª–∏—Ç –Ω–µ train, –ø—Ä–æ–ø—É—Å–∫–∞–µ–º chain-merge
            logging.debug("–§–∞–π–ª –Ω–µ –≤—ã–±—Ä–∞–Ω –¥–ª—è —Å–∫–ª–µ–π–∫–∏ (–∏–ª–∏ –Ω–µ train), –ø—Ä–æ–ø—É—Å–∫–∞–µ–º chain merge.")

        if was_merged:
            logging.debug("üìù –¢–µ–∫—Å—Ç: –∞—É–¥–∏–æ –±—ã–ª–æ merged ‚Äì –≤—ã–∑—ã–≤–∞–µ–º Whisper.")
            text_final = self.run_whisper(waveform)
            logging.debug(f"üÜï Whisper –ø—Ä–µ–¥—Å–∫–∞–∑–∞–ª: {text_final}")

            merge_components = [os.path.splitext(os.path.basename(audio_path))[0]]
            merge_components += [os.path.splitext(os.path.basename(p))[0] for p in used_candidates]

            self.whisper_csv_update_log.append({
                "video_name": os.path.splitext(os.path.basename(audio_path))[0],
                "text_new": text_final,
                "text_old": csv_text,
                "was_merged": True,
                "merge_components": merge_components
            })

        else:
            if csv_text.strip():
                logging.debug("–¢–µ–∫—Å—Ç: –∏—Å–ø–æ–ª—å–∑—É–µ–º CSV-—Ç–µ–∫—Å—Ç (–Ω–µ –ø—É—Å—Ç).")
                text_final = csv_text
            else:
                if self.split == "train" or self.use_whisper_for_nontrain_if_no_text:
                    logging.debug("–¢–µ–∫—Å—Ç: CSV –ø—É—Å—Ç–æ–π ‚Äì –≤—ã–∑—ã–≤–∞–µ–º Whisper.")
                    text_final = self.run_whisper(waveform)
                else:
                    logging.debug("–¢–µ–∫—Å—Ç: CSV –ø—É—Å—Ç–æ–π –∏ –Ω–µ –≤—ã–∑—ã–≤–∞–µ–º Whisper –¥–ª—è dev/test.")
                    text_final = ""

        audio_pred, audion_emb = self.audio_feature_extractor.extract(waveform[0], self.sample_rate)
        text_pred, text_emb = self.text_feature_extractor.extract(text_final)

        return {
            "audio_path": os.path.basename(audio_path),
            "audio": audion_emb[0],
            "label": label_vec,
            "text": text_emb[0],
            "audio_pred": audio_pred[0],
            "text_pred": text_pred[0]
        }

    def prepare_data(self):
        """
        –ó–∞–≥—Ä—É–∂–∞–µ—Ç –∏ –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç –æ–¥–∏–Ω —ç–ª–µ–º–µ–Ω—Ç –¥–∞—Ç–∞—Å–µ—Ç–∞,
        —Å–æ—Ö—Ä–∞–Ω—è–µ—Ç —ç–º–±–µ–¥–¥–∏–Ω–≥–∏ –∏ –æ–±–Ω–æ–≤–ª—ë–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç (–µ—Å–ª–∏ –±—ã–ª–æ —Å–∫–ª–µ–µ–Ω–æ).
        """
        for idx, row in enumerate(tqdm(self.rows)):
            curr_dict = self.get_data(row)
            if curr_dict is not None:
                self.meta.append(curr_dict)

        # === –°–æ—Ö—Ä–∞–Ω—è–µ–º CSV —Å –æ–±–Ω–æ–≤–ª—ë–Ω–Ω—ã–º–∏ —Ç–µ–∫—Å—Ç–∞–º–∏ (—Ç–æ–ª—å–∫–æ –µ—Å–ª–∏ –±—ã–ª merge) ===
        if self.whisper_csv_update_log:
            df_log = pd.DataFrame(self.whisper_csv_update_log)

            # –ö–æ–ø–∏—è –∏—Å—Ö–æ–¥–Ω–æ–≥–æ CSV
            df_out = self.original_df.copy()

            # –ú–µ—Ä–∂–∏–º –ø–æ video_name
            df_out = df_out.merge(df_log, on="video_name", how="left")

            # –û–±–Ω–æ–≤–ª—è–µ–º —Ç–µ–∫—Å—Ç: –∑–∞–º–µ–Ω—è–µ–º —Ç–æ–ª—å–∫–æ –µ—Å–ª–∏ Whisper —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–ª
            df_out["text_final"] = df_out["text_new"].combine_first(df_out["text"])
            df_out["text_old"] = df_out["text"]
            df_out["text"] = df_out["text_final"]
            df_out["was_merged"] = df_out["was_merged"].fillna(False).astype(bool)

            # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º merge_components –≤ —Å—Ç—Ä–æ–∫—É
            df_out["merge_components"] = df_out["merge_components"].apply(
                lambda x: ";".join(x) if isinstance(x, list) else ""
            )

            # –ß–∏—Å—Ç–∏–º –≤—Ä–µ–º–µ–Ω–Ω—ã–µ –∫–æ–ª–æ–Ω–∫–∏
            df_out = df_out.drop(columns=["text_new", "text_final"])

            # –°–æ—Ö—Ä–∞–Ω—è–µ–º –∫–∞–∫ CSV
            output_path = os.path.join(self.save_feature_path, f"{self.dataset_name}_{self.split}_merged_whisper_{self.merge_probability *100}.csv")
            os.makedirs(self.save_feature_path, exist_ok=True)
            df_out.to_csv(output_path, index=False, encoding="utf-8")
            logging.info(f"üìÑ –û–±–Ω–æ–≤–ª—ë–Ω–Ω—ã–π merged CSV —Å–æ—Ö—Ä–∞–Ω—ë–Ω: {output_path}")

    def __getitem__(self, index):
        if self.save_prepared_data:
            return self.meta[index]
        else:
            return self.get_data(self.rows[index])

    def load_audio(self, path):
        """
        –ó–∞–≥—Ä—É–∂–∞–µ—Ç –∞—É–¥–∏–æ –ø–æ —É–∫–∞–∑–∞–Ω–Ω–æ–º—É –ø—É—Ç–∏ –∏ —Ä–µ—Å—ç–º–ø–ª–∏—Ä—É–µ—Ç –µ–≥–æ –¥–æ self.sample_rate, –µ—Å–ª–∏ –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ.
        """
        if not os.path.exists(path):
            logging.warning(f"–§–∞–π–ª –æ—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç: {path}")
            return None, None
        try:
            wf, sr = torchaudio.load(path)
            if sr != self.sample_rate:
                resampler = torchaudio.transforms.Resample(sr, self.sample_rate)
                wf = resampler(wf)
                sr = self.sample_rate
            return wf, sr
        except Exception as e:
            logging.error(f"–û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ {path}: {e}")
            return None, None

    def get_suitable_audio(self, label_name, exclude_path, min_needed, top_k=5):
        """
        –ò—â–µ—Ç –∞—É–¥–∏–æ—Ñ–∞–π–ª —Å —Ç–æ–π –∂–µ —ç–º–æ—Ü–∏–µ–π.
        1) –ï—Å–ª–∏ –µ—Å—Ç—å —Ñ–∞–π–ª—ã >= min_needed, –≤—ã–±–∏—Ä–∞–µ–º —Å–ª—É—á–∞–π–Ω–æ –∏–∑ –Ω–∏—Ö.
        2) –ï—Å–ª–∏ —Ç–∞–∫–∏—Ö –Ω–µ—Ç, –±–µ—Ä—ë–º —Ç–æ–ø-K —Å–∞–º—ã—Ö –¥–ª–∏–Ω–Ω—ã—Ö, –ø–æ—Ç–æ–º –∏–∑ –Ω–∏—Ö –±–µ—Ä—ë–º —Å–ª—É—á–∞–π–Ω—ã–π.
        """

        candidates = [p for p, lbl in self.audio_class_map.items()
                    if lbl == label_name and p != exclude_path]
        logging.debug(f"üîç –ù–∞–π–¥–µ–Ω–æ {len(candidates)} –∫–∞–Ω–¥–∏–¥–∞—Ç–æ–≤ –¥–ª—è —ç–º–æ—Ü–∏–∏ '{label_name}'")

        # –°–æ—Ö—Ä–∞–Ω–∏–º: (eq_len, path) –¥–ª—è –≤—Å–µ—Ö –∫–∞–Ω–¥–∏–¥–∞—Ç–æ–≤, –Ω–æ –ë–ï–ó –ø–æ–≤—Ç–æ—Ä–Ω–æ–≥–æ —á—Ç–µ–Ω–∏—è torchaudio.info
        all_info = []
        for path in candidates:
            # <<< NEW: –≤–º–µ—Å—Ç–æ info = torchaudio.info(path) ...
            eq_len = self.path_info.get(path, 0)  # –ü–æ–ª—É—á–∞–µ–º –∏–∑ –∫—ç—à–∞
            all_info.append((eq_len, path))

        valid = [(l, p) for l, p in all_info if l >= min_needed]
        logging.debug(f"‚úÖ –ü–æ–¥—Ö–æ–¥—è—â–∏—Ö (>= {min_needed}): {len(valid)} (–∏–∑ {len(all_info)})")

        if valid:
            # –ï—Å–ª–∏ –µ—Å—Ç—å –∏–¥–µ–∞–ª—å–Ω—ã–µ ‚Äî –±–µ—Ä—ë–º —Å–ª—É—á–∞–π–Ω–æ –∏–∑ –Ω–∏—Ö
            random.shuffle(valid)
            chosen = random.choice(valid)[1]
            return chosen
        else:
            # 2) –ï—Å–ª–∏ –∏–¥–µ–∞–ª—å–Ω—ã—Ö –Ω–µ—Ç ‚Äî –±–µ—Ä—ë–º —Ç–æ–ø-K –ø–æ –¥–ª–∏–Ω–µ
            sorted_by_len = sorted(all_info, key=lambda x: x[0], reverse=True)
            top_k_list = sorted_by_len[:top_k]
            if not top_k_list:
                logging.debug("–ù–µ—Ç –¥–æ—Å—Ç—É–ø–Ω—ã—Ö –∫–∞–Ω–¥–∏–¥–∞—Ç–æ–≤ –≤–æ–æ–±—â–µ.")
                return None  # –≤–æ–æ–±—â–µ –Ω–µ—Ç –∫–∞–Ω–¥–∏–¥–∞—Ç–æ–≤

            random.shuffle(top_k_list)
            chosen = top_k_list[0][1]
            logging.info(f"–ò–∑ —Ç–æ–ø-{top_k} –≤—ã–±—Ä–∞–Ω –∫–∞–Ω–¥–∏–¥–∞—Ç: {chosen}")
            return chosen

    def run_whisper(self, waveform):
        """
        –í—ã–∑—ã–≤–∞–µ—Ç Whisper –Ω–∞ –∞—É–¥–∏–æ—Å–∏–≥–Ω–∞–ª–µ –∏ –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç –ø–æ–ª–Ω—ã–π —Ç–µ–∫—Å—Ç (–±–µ–∑ –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏—è –ø–æ –∫–æ–ª–∏—á–µ—Å—Ç–≤—É —Å–ª–æ–≤).
        """
        arr = waveform.squeeze().cpu().numpy()
        try:
            with torch.no_grad():
                result = self.whisper_model.transcribe(arr, fp16=False)
            text = result["text"].strip()
            return text
        except Exception as e:
            logging.error(f"Whisper –æ—à–∏–±–∫–∞: {e}")
            return ""

    def _add_synthetic_data(self, synthetic_ratio):
        """
        –î–æ–±–∞–≤–ª—è–µ—Ç synthetic_ratio (0..1) –æ—Ç –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ –¥–æ—Å—Ç—É–ø–Ω—ã—Ö —Å–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–∏—Ö —Ñ–∞–π–ª–æ–≤ –Ω–∞ –∫–∞–∂–¥—É—é —ç–º–æ—Ü–∏—é.
        """
        if not self.synthetic_path:
            logging.warning("‚ö† –ü—É—Ç—å –∫ —Å–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–∏–º –¥–∞–Ω–Ω—ã–º –Ω–µ —É–∫–∞–∑–∞–Ω.")
            return

        random.seed(self.seed)

        synth_csv_path = os.path.join(self.synthetic_path, "meld_s_train_labels.csv")
        synth_wav_dir = os.path.join(self.synthetic_path, "wavs")

        if not (os.path.exists(synth_csv_path) and os.path.exists(synth_wav_dir)):
            logging.warning("‚ö† –°–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–∏–µ –¥–∞–Ω–Ω—ã–µ –Ω–µ –Ω–∞–π–¥–µ–Ω—ã.")
            return

        df_synth = pd.read_csv(synth_csv_path)
        rows_by_label = {emotion: [] for emotion in self.emotion_columns}

        for _, row in df_synth.iterrows():
            audio_path = os.path.join(synth_wav_dir, f"{row['video_name']}.wav")
            if not os.path.exists(audio_path):
                continue
            emotion_values = row[self.emotion_columns].values.astype(float)
            max_idx = np.argmax(emotion_values)
            label = self.emotion_columns[max_idx]
            csv_text = row[self.text_column] if self.text_column in row and isinstance(row[self.text_column], str) else ""
            rows_by_label[label].append({
                "audio_path": audio_path,
                "label": label,
                "csv_text": csv_text
            })

        added = 0
        for label in self.emotion_columns:
            candidates = rows_by_label[label]
            if not candidates:
                continue
            count_synth = int(len(candidates) * synthetic_ratio)
            if count_synth <= 0:
                continue
            selected = random.sample(candidates, count_synth)
            self.rows.extend(selected)
            added += len(selected)
            logging.info(f"‚ûï –î–æ–±–∞–≤–ª–µ–Ω–æ {len(selected)} —Å–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–∏—Ö –ø—Ä–∏–º–µ—Ä–æ–≤ –¥–ª—è —ç–º–æ—Ü–∏–∏ '{label}'")

        logging.info(f"üì¶ –í—Å–µ–≥–æ –¥–æ–±–∞–≤–ª–µ–Ω–æ {added} —Å–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–∏—Ö –ø—Ä–∏–º–µ—Ä–æ–≤ –∏–∑ MELD_S")

    def emotion_to_vector(self, label_name):
        """
        –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ—Ç –Ω–∞–∑–≤–∞–Ω–∏–µ —ç–º–æ—Ü–∏–∏ –≤ one-hot –≤–µ–∫—Ç–æ—Ä (torch.tensor).
        """
        v = np.zeros(len(self.emotion_columns), dtype=np.float32)
        if label_name in self.emotion_columns:
            idx = self.emotion_columns.index(label_name)
            v[idx] = 1.0
        return torch.tensor(v, dtype=torch.float32)

class DatasetMultiModal(Dataset):
    """
    –ú—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã–π –¥–∞—Ç–∞—Å–µ—Ç –¥–ª—è –∞—É–¥–∏–æ, —Ç–µ–∫—Å—Ç–∞ –∏ —ç–º–æ—Ü–∏–π (–æ–Ω‚Äëthe‚Äëfly –≤–µ—Ä—Å–∏—è).

    –ü—Ä–∏ –∫–∞–∂–¥–æ–º –≤—ã–∑–æ–≤–µ __getitem__:
      - –ó–∞–≥—Ä—É–∂–∞–µ—Ç WAV –ø–æ video_name –∏–∑ CSV.
      - –î–ª—è –æ–±—É—á–∞—é—â–µ–π –≤—ã–±–æ—Ä–∫–∏ (split="train"):
            –ï—Å–ª–∏ –∞—É–¥–∏–æ –∫–æ—Ä–æ—á–µ target_samples, –ø—Ä–æ–≤–µ—Ä—è–µ–º, –≤—ã–±—Ä–∞–ª–∏ –ª–∏ –º—ã —ç—Ç–æ—Ç —Ñ–∞–π–ª –¥–ª—è —Å–∫–ª–µ–π–∫–∏
            (–ø–æ merge_probability). –ï—Å–ª–∏ –¥–∞ ‚Äì –≤—ã–ø–æ–ª–Ω—è–µ—Ç—Å—è "chain merge":
            –≤—ã–±–∏—Ä–∞–µ—Ç—Å—è –æ–¥–∏–Ω –∏–ª–∏ –Ω–µ—Å–∫–æ–ª—å–∫–æ –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã—Ö —Ñ–∞–π–ª–æ–≤ —Ç–æ–≥–æ –∂–µ –∫–ª–∞—Å—Å–∞, –¥–∞–∂–µ –µ—Å–ª–∏ –æ–¥–∏–Ω –∫–∞–Ω–¥–∏–¥–∞—Ç –¥–ª–∏–Ω–Ω–µ–µ,
            –∏ –∏—Ç–æ–≥–æ–≤–æ–µ –∞—É–¥–∏–æ –∑–∞—Ç–µ–º –æ–±—Ä–µ–∑–∞–µ—Ç—Å—è –¥–æ —Ç–æ—á–Ω–æ–π –¥–ª–∏–Ω—ã.
      - –ï—Å–ª–∏ –∏—Ç–æ–≥–æ–≤–æ–µ –∞—É–¥–∏–æ –≤—Å—ë –µ—â—ë –º–µ–Ω—å—à–µ target_samples, –≤—ã–ø–æ–ª–Ω—è–µ—Ç—Å—è –ø–∞–¥–¥–∏–Ω–≥ –Ω—É–ª—è–º–∏.
      - –¢–µ–∫—Å—Ç –≤—ã–±–∏—Ä–∞–µ—Ç—Å—è —Ç–∞–∫:
            ‚Ä¢ –ï—Å–ª–∏ –∞—É–¥–∏–æ –±—ã–ª–æ merged (—Å–∫–ª–µ–µ–Ω–æ) ‚Äì –≤—ã–∑—ã–≤–∞–µ—Ç—Å—è Whisper –¥–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è –Ω–æ–≤–æ–≥–æ —Ç–µ–∫—Å—Ç–∞.
            ‚Ä¢ –ï—Å–ª–∏ merge –Ω–µ –ø—Ä–æ–∏—Å—Ö–æ–¥–∏–ª–æ –∏ CSV-—Ç–µ–∫—Å—Ç –Ω–µ –ø—É—Å—Ç ‚Äì –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è CSV-—Ç–µ–∫—Å—Ç.
            ‚Ä¢ –ï—Å–ª–∏ CSV-—Ç–µ–∫—Å—Ç –ø—É—Å—Ç–æ–π ‚Äì –¥–ª—è train (–∏–ª–∏, –ø—Ä–∏ —É—Å–ª–æ–≤–∏–∏, –¥–ª—è dev/test) –≤—ã–∑—ã–≤–∞–µ—Ç—Å—è Whisper.
      - –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å–ª–æ–≤–∞—Ä—å { "audio": waveform, "label": label_vector, "text": text_final }.
    """

    def __init__(
        self,
        csv_path,
        wav_dir,
        emotion_columns,
        split="train",
        sample_rate=16000,
        wav_length=4,
        whisper_model="tiny",
        text_column="text",
        use_whisper_for_nontrain_if_no_text=True,
        whisper_device="cuda",
        subset_size=0,
        merge_probability=1.0  # <-- –ù–æ–≤—ã–π –ø–∞—Ä–∞–º–µ—Ç—Ä: –¥–æ–ª—è –æ—Ç –û–ë–©–ï–ì–û —á–∏—Å–ª–∞ —Ñ–∞–π–ª–æ–≤
    ):
        """
        :param csv_path: –ü—É—Ç—å –∫ CSV-—Ñ–∞–π–ª—É (—Å –∫–æ–ª–æ–Ω–∫–∞–º–∏ video_name, emotion_columns, –≤–æ–∑–º–æ–∂–Ω–æ text).
        :param wav_dir: –ü–∞–ø–∫–∞ —Å –∞—É–¥–∏–æ—Ñ–∞–π–ª–∞–º–∏ (–∏–º—è —Ñ–∞–π–ª–∞: video_name.wav).
        :param emotion_columns: –°–ø–∏—Å–æ–∫ –∫–æ–ª–æ–Ω–æ–∫ —ç–º–æ—Ü–∏–π, –Ω–∞–ø—Ä–∏–º–µ—Ä ["neutral", "happy", "sad", ...].
        :param split: "train", "dev" –∏–ª–∏ "test".
        :param sample_rate: –¶–µ–ª–µ–≤–∞—è —á–∞—Å—Ç–æ—Ç–∞ –¥–∏—Å–∫—Ä–µ—Ç–∏–∑–∞—Ü–∏–∏ (–Ω–∞–ø—Ä–∏–º–µ—Ä, 16000).
        :param wav_length: –¶–µ–ª–µ–≤–∞—è –¥–ª–∏–Ω–∞ –∞—É–¥–∏–æ –≤ —Å–µ–∫—É–Ω–¥–∞—Ö.
        :param whisper_model: –ù–∞–∑–≤–∞–Ω–∏–µ –º–æ–¥–µ–ª–∏ Whisper ("tiny", "base", "small", ...).
        :param max_text_tokens: (–ù–µ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è) ‚Äì –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–µ –Ω–∞ —á–∏—Å–ª–æ —Ç–æ–∫–µ–Ω–æ–≤.
        :param text_column: –ù–∞–∑–≤–∞–Ω–∏–µ –∫–æ–ª–æ–Ω–∫–∏ —Å —Ç–µ–∫—Å—Ç–æ–º –≤ CSV.
        :param use_whisper_for_nontrain_if_no_text: –ï—Å–ª–∏ True, –¥–ª—è dev/test –ø—Ä–∏ –æ—Ç—Å—É—Ç—Å—Ç–≤–∏–∏ CSV-—Ç–µ–∫—Å—Ç–∞ –≤—ã–∑—ã–≤–∞–µ—Ç—Å—è Whisper.
        :param whisper_device: "cuda" –∏–ª–∏ "cpu" ‚Äì —É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ –¥–ª—è –º–æ–¥–µ–ª–∏ Whisper.
        :param subset_size: –ï—Å–ª–∏ > 0, –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è —Ç–æ–ª—å–∫–æ –ø–µ—Ä–≤—ã–µ N –∑–∞–ø–∏—Å–µ–π –∏–∑ CSV (–¥–ª—è –æ—Ç–ª–∞–¥–∫–∏).
        :param merge_probability: –ü—Ä–æ—Ü–µ–Ω—Ç (0..1) –æ—Ç –≤—Å–µ–≥–æ —á–∏—Å–ª–∞ —Ñ–∞–π–ª–æ–≤, –∫–æ—Ç–æ—Ä—ã–µ –±—É–¥—É—Ç —Å–∫–ª–µ–∏–≤–∞—Ç—å—Å—è, –µ—Å–ª–∏ –æ–Ω–∏ –∫–æ—Ä–æ—á–µ.
        """
        super().__init__()
        self.split = split
        self.sample_rate = sample_rate
        self.target_samples = int(wav_length * sample_rate)
        self.emotion_columns = emotion_columns
        self.whisper_model_name = whisper_model
        self.text_column = text_column
        self.use_whisper_for_nontrain_if_no_text = use_whisper_for_nontrain_if_no_text
        self.whisper_device = whisper_device
        self.merge_probability = merge_probability

        # –ó–∞–≥—Ä—É–∂–∞–µ–º CSV
        if not os.path.exists(csv_path):
            raise ValueError(f"–û—à–∏–±–∫–∞: —Ñ–∞–π–ª CSV –Ω–µ –Ω–∞–π–¥–µ–Ω: {csv_path}")
        df = pd.read_csv(csv_path)
        if subset_size > 0:
            df = df.head(subset_size)
            logging.info(f"[DatasetMultiModal] –ò—Å–ø–æ–ª—å–∑—É–µ–º —Ç–æ–ª—å–∫–æ –ø–µ—Ä–≤—ã–µ {len(df)} –∑–∞–ø–∏—Å–µ–π (subset_size={subset_size}).")

        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ –≤—Å–µ—Ö –∫–æ–ª–æ–Ω–æ–∫ —ç–º–æ—Ü–∏–π
        missing = [c for c in emotion_columns if c not in df.columns]
        if missing:
            raise ValueError(f"–í CSV –æ—Ç—Å—É—Ç—Å—Ç–≤—É—é—Ç –Ω–µ–æ–±—Ö–æ–¥–∏–º—ã–µ –∫–æ–ª–æ–Ω–∫–∏ —ç–º–æ—Ü–∏–π: {missing}")

        # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å—É—â–µ—Å—Ç–≤–æ–≤–∞–Ω–∏–µ –ø–∞–ø–∫–∏ —Å –∞—É–¥–∏–æ
        if not os.path.exists(wav_dir):
            raise ValueError(f"–û—à–∏–±–∫–∞: –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—è —Å –∞—É–¥–∏–æ {wav_dir} –Ω–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç!")
        self.wav_dir = wav_dir

        # –°–æ–±–∏—Ä–∞–µ–º —Å–ø–∏—Å–æ–∫ —Å—Ç—Ä–æ–∫: –¥–ª—è –∫–∞–∂–¥–æ–π –∑–∞–ø–∏—Å–∏ –ø–æ–ª—É—á–∞–µ–º –ø—É—Ç—å –∫ –∞—É–¥–∏–æ, label –∏ CSV-—Ç–µ–∫—Å—Ç (–µ—Å–ª–∏ –µ—Å—Ç—å)
        self.rows = []
        for i, rowi in df.iterrows():
            audio_path = os.path.join(wav_dir, f"{rowi['video_name']}.wav")
            if not os.path.exists(audio_path):
                continue
            # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –¥–æ–º–∏–Ω–∏—Ä—É—é—â—É—é —ç–º–æ—Ü–∏—é (–º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∑–Ω–∞—á–µ–Ω–∏–µ)
            emotion_values = rowi[self.emotion_columns].values.astype(float)
            max_idx = np.argmax(emotion_values)
            emotion_label = self.emotion_columns[max_idx]

            # –ò–∑–≤–ª–µ–∫–∞–µ–º —Ç–µ–∫—Å—Ç –∏–∑ CSV (–µ—Å–ª–∏ –µ—Å—Ç—å)
            csv_text = ""
            if self.text_column in rowi and isinstance(rowi[self.text_column], str):
                csv_text = rowi[self.text_column]

            self.rows.append({
                "audio_path": audio_path,
                "label": emotion_label,
                "csv_text": csv_text
            })

        # –°–æ–∑–¥–∞–µ–º –∫–∞—Ä—Ç—É –¥–ª—è –ø–æ–∏—Å–∫–∞ —Ñ–∞–π–ª–æ–≤ –ø–æ —ç–º–æ—Ü–∏–∏
        self.audio_class_map = {entry["audio_path"]: entry["label"] for entry in self.rows}

        logging.info("üìä –ê–Ω–∞–ª–∏–∑ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è —Ñ–∞–π–ª–æ–≤ –ø–æ —ç–º–æ—Ü–∏—è–º:")
        emotion_counts = {emotion: 0 for emotion in set(self.audio_class_map.values())}
        for path, emotion in self.audio_class_map.items():
            emotion_counts[emotion] += 1
        for emotion, count in emotion_counts.items():
            logging.info(f"üé≠ –≠–º–æ—Ü–∏—è '{emotion}': {count} —Ñ–∞–π–ª–æ–≤.")

        logging.info(f"[DatasetMultiModal] –°–ø–ª–∏—Ç={split}, –≤—Å–µ–≥–æ —Å—Ç—Ä–æ–∫: {len(self.rows)}")

        # === –ü—Ä–æ—Ü–µ–Ω—Ç–Ω–æ–µ —Å–µ–º–ø–ª–∏—Ä–æ–≤–∞–Ω–∏–µ ===
        total_files = len(self.rows)
        num_to_merge = int(total_files * self.merge_probability)

        # <<< NEW: –ö–µ—à–∏—Ä—É–µ–º –¥–ª–∏–Ω—ã (eq_len) –¥–ª—è –≤—Å–µ—Ö —Ñ–∞–π–ª–æ–≤ >>>
        self.path_info = {}
        for row in self.rows:
            p = row["audio_path"]
            try:
                info = torchaudio.info(p)
                length = info.num_frames
                sr_ = info.sample_rate
                # –ø–µ—Ä–µ–≤–æ–¥–∏–º –¥–ª–∏–Ω—É –≤ "—ç–∫–≤–∏–≤–∞–ª–µ–Ω—Ç self.sample_rate"
                if sr_ != self.sample_rate:
                    ratio = sr_ / self.sample_rate
                    eq_len = int(length / ratio)
                else:
                    eq_len = length
                self.path_info[p] = eq_len
            except Exception as e:
                logging.warning(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ —á—Ç–µ–Ω–∏—è {p}: {e}")
                self.path_info[p] = 0  # –ï—Å–ª–∏ –Ω–µ —Å–º–æ–≥–ª–∏ –ø—Ä–æ—á–∏—Ç–∞—Ç—å, —Å—Ç–∞–≤–∏–º 0

        # –û–ø—Ä–µ–¥–µ–ª–∏–º, –∫–∞–∫–∏–µ —Ñ–∞–π–ª—ã "–∫–æ—Ä–æ—Ç–∫–∏–µ" (–º–æ–≥—É—Ç –Ω—É–∂–¥–∞—Ç—å—Å—è –≤ —Å–∫–ª–µ–π–∫–µ) - –∏—Å–ø–æ–ª—å–∑—É–µ–º –∫—ç—à –≤–º–µ—Å—Ç–æ —Å—Ç–∞—Ä–æ–≥–æ _is_too_short
        self.mergable_files = [
            row["audio_path"]  # –≤–º–µ—Å—Ç–æ —Ü–µ–ª–æ–≥–æ dict –±–µ—Ä—ë–º —Å—Ç—Ä–æ–∫—É
            for row in self.rows
            if self._is_too_short_cached(row["audio_path"])  # <<< —Ç–µ–ø–µ—Ä—å —Ç—É—Ç –∏—Å–ø–æ–ª—å–∑—É–µ–º –Ω–æ–≤—É—é —Ñ—É–Ω–∫—Ü–∏—é
        ]
        short_count = len(self.mergable_files)

        # –ï—Å–ª–∏ –∫–æ—Ä–æ—Ç–∫–∏—Ö —Ñ–∞–π–ª–æ–≤ –±–æ–ª—å—à–µ –Ω—É–∂–Ω–æ–≥–æ —á–∏—Å–ª–∞, –≤—ã–±–µ—Ä–µ–º —Å–ª—É—á–∞–π–Ω—ã–µ. –ò–Ω–∞—á–µ –≤—Å–µ –∫–æ—Ä–æ—Ç–∫–∏–µ.
        if short_count > num_to_merge:
            self.files_to_merge = set(random.sample(self.mergable_files, num_to_merge))
        else:
            self.files_to_merge = set(self.mergable_files)

        logging.info(f"üîó –í—Å–µ–≥–æ —Ñ–∞–π–ª–æ–≤: {total_files}, –Ω—É–∂–Ω–æ —Å–∫–ª–µ–∏—Ç—å: {num_to_merge} ({self.merge_probability*100:.0f}%)")
        logging.info(f"üîó –ö–æ—Ä–æ—Ç–∫–∏—Ö —Ñ–∞–π–ª–æ–≤: {short_count}, –≤—ã–±—Ä–∞–Ω–æ –¥–ª—è —Å–∫–ª–µ–π–∫–∏: {len(self.files_to_merge)}")

        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º Whisper-–º–æ–¥–µ–ª—å –æ–¥–∏–Ω —Ä–∞–∑
        logging.info(f"–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è Whisper: –º–æ–¥–µ–ª—å={whisper_model}, —É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ={whisper_device}")
        self.whisper_model = whisper.load_model(whisper_model, device=whisper_device).eval()
        # print(f"üì¶ Whisper —Ä–∞–±–æ—Ç–∞–µ—Ç –Ω–∞ —É—Å—Ç—Ä–æ–π—Å—Ç–≤–µ: {self.whisper_model.device}")

    def _is_too_short(self, audio_path):
        """
        (–û—Ä–∏–≥–∏–Ω–∞–ª—å–Ω–∞—è) –ü—Ä–æ–≤–µ—Ä—è–µ–º, —è–≤–ª—è–µ—Ç—Å—è –ª–∏ —Ñ–∞–π–ª –∫–æ—Ä–æ—á–µ target_samples.
        –ò—Å–ø–æ–ª—å–∑—É–µ—Ç torchaudio.info(audio_path).
        –ù–æ —Ç–µ–ø–µ—Ä—å —ç—Ç–æ—Ç –º–µ—Ç–æ–¥ –Ω–µ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è, –ø–æ—Å–∫–æ–ª—å–∫—É –º—ã –∫–µ—à–∏—Ä—É–µ–º –¥–ª–∏–Ω—ã.
        """
        try:
            info = torchaudio.info(audio_path)
            length = info.num_frames
            sr_ = info.sample_rate
            # –ø–µ—Ä–µ–≤–æ–¥–∏–º –¥–ª–∏–Ω—É –≤ "—ç–∫–≤–∏–≤–∞–ª–µ–Ω—Ç self.sample_rate"
            if sr_ != self.sample_rate:
                ratio = sr_ / self.sample_rate
                eq_len = int(length / ratio)
            else:
                eq_len = length
            return eq_len < self.target_samples
        except Exception as e:
            logging.warning(f"–û—à–∏–±–∫–∞ _is_too_short({audio_path}): {e}")
            return False

    def _is_too_short_cached(self, audio_path):
        """
        (–ù–æ–≤–∞—è) –ü—Ä–æ–≤–µ—Ä—è–µ–º, —è–≤–ª—è–µ—Ç—Å—è –ª–∏ —Ñ–∞–π–ª –∫–æ—Ä–æ—á–µ target_samples, –∏—Å–ø–æ–ª—å–∑—É—è –∑–∞–∫–µ—à–∏—Ä–æ–≤–∞–Ω–Ω—É—é –¥–ª–∏–Ω—É –≤ self.path_info.
        """
        eq_len = self.path_info.get(audio_path, 0)
        return eq_len < self.target_samples

    def __len__(self):
        return len(self.rows)

    def __getitem__(self, index):
        """
        –ó–∞–≥—Ä—É–∂–∞–µ—Ç –∏ –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç –æ–¥–∏–Ω —ç–ª–µ–º–µ–Ω—Ç –¥–∞—Ç–∞—Å–µ—Ç–∞ (–æ–Ω‚Äëthe‚Äëfly).
        """
        row = self.rows[index]
        audio_path = row["audio_path"]
        label_name = row["label"]
        csv_text = row["csv_text"]

        # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º label –≤ one-hot –≤–µ–∫—Ç–æ—Ä
        label_vec = self.emotion_to_vector(label_name)

        # –®–∞–≥ 1. –ó–∞–≥—Ä—É–∂–∞–µ–º –∞—É–¥–∏–æ
        waveform, sr = self.load_audio(audio_path)
        if waveform is None:
            return None

        orig_len = waveform.shape[1]
        logging.debug(f"–ò—Å—Ö–æ–¥–Ω–∞—è –¥–ª–∏–Ω–∞ {os.path.basename(audio_path)}: {orig_len/sr:.2f} —Å–µ–∫")

        was_merged = False
        merged_texts = [csv_text]  # –¢–µ–∫—Å—Ç—ã –∏—Å—Ö–æ–¥–Ω–æ–≥–æ —Ñ–∞–π–ª–∞ + –¥–æ–±–∞–≤–ª–µ–Ω–Ω—ã—Ö

        # –®–∞–≥ 2. –î–ª—è train, –µ—Å–ª–∏ –∞—É–¥–∏–æ –∫–æ—Ä–æ—á–µ target_samples, –ø—Ä–æ–≤–µ—Ä—è–µ–º:
        #        –ø–æ–ø–∞–ª –ª–∏ –¥–∞–Ω–Ω—ã–π row –≤ files_to_merge?
        if self.split == "train" and row["audio_path"] in self.files_to_merge:
            # chain merge
            current_length = orig_len
            used_candidates = set()

            while current_length < self.target_samples:
                needed = self.target_samples - current_length
                candidate = self.get_suitable_audio(label_name, exclude_path=audio_path, min_needed=needed, top_k=10)
                if candidate is None or candidate in used_candidates:
                    break
                used_candidates.add(candidate)
                add_wf, add_sr = self.load_audio(candidate)
                if add_wf is None:
                    break
                logging.debug(f"–°–∫–ª–µ–π–∫–∞: –¥–æ–±–∞–≤–ª—è–µ–º {os.path.basename(candidate)} (–Ω–µ–æ–±—Ö–æ–¥–∏–º—ã—Ö —Å—ç–º–ø–ª–æ–≤: {needed})")
                waveform = torch.cat((waveform, add_wf), dim=1)
                current_length = waveform.shape[1]
                was_merged = True

                # –ü–æ–ª—É—á–∞–µ–º —Ç–µ–∫—Å—Ç –≤—Ç–æ—Ä–æ–≥–æ —Ñ–∞–π–ª–∞ (–µ—Å–ª–∏ –µ—Å—Ç—å –≤ CSV)
                add_csv_text = next((r["csv_text"] for r in self.rows if r["audio_path"] == candidate), "")
                merged_texts.append(add_csv_text)

                logging.debug(f"üìú –¢–µ–∫—Å—Ç –ø–µ—Ä–≤–æ–≥–æ —Ñ–∞–π–ª–∞: {csv_text}")
                logging.debug(f"üìú –¢–µ–∫—Å—Ç –¥–æ–±–∞–≤–ª–µ–Ω–Ω–æ–≥–æ —Ñ–∞–π–ª–∞: {add_csv_text}")
        else:
            # –ï—Å–ª–∏ —Ñ–∞–π–ª –Ω–µ –≤ —Å–ø–∏—Å–∫–µ "–¥–æ–ª–∂–Ω—ã —Å–∫–ª–µ–∏—Ç—å" –∏–ª–∏ —Å–ø–ª–∏—Ç –Ω–µ train, –ø—Ä–æ–ø—É—Å–∫–∞–µ–º chain-merge
            logging.debug("–§–∞–π–ª –Ω–µ –≤—ã–±—Ä–∞–Ω –¥–ª—è —Å–∫–ª–µ–π–∫–∏ (–∏–ª–∏ –Ω–µ train), –ø—Ä–æ–ø—É—Å–∫–∞–µ–º chain merge.")

        # –®–∞–≥ 3. –ï—Å–ª–∏ –∏—Ç–æ–≥–æ–≤–∞—è –¥–ª–∏–Ω–∞ –º–µ–Ω—å—à–µ target_samples, –ø–∞–¥–¥–∏–Ω–≥ –Ω—É–ª—è–º–∏
        curr_len = waveform.shape[1]
        if curr_len < self.target_samples:
            pad_size = self.target_samples - curr_len
            logging.debug(f"–ü–∞–¥–¥–∏–Ω–≥ {os.path.basename(audio_path)}: +{pad_size} —Å—ç–º–ø–ª–æ–≤")
            waveform = torch.nn.functional.pad(waveform, (0, pad_size))

        # –®–∞–≥ 4. –û–±—Ä–µ–∑–∞–µ–º –∞—É–¥–∏–æ –¥–æ target_samples (–µ—Å–ª–∏ –≤—ã—à–ª–æ –±–æ–ª—å—à–µ)
        waveform = waveform[:, :self.target_samples]
        logging.debug(f"–§–∏–Ω–∞–ª—å–Ω–∞—è –¥–ª–∏–Ω–∞ {os.path.basename(audio_path)}: {waveform.shape[1]/sr:.2f} —Å–µ–∫; was_merged={was_merged}")

        # –®–∞–≥ 5. –ü–æ–ª—É—á–∞–µ–º —Ç–µ–∫—Å—Ç
        if was_merged:
            logging.debug("üìù –¢–µ–∫—Å—Ç: –∞—É–¥–∏–æ –±—ã–ª–æ merged ‚Äì –≤—ã–∑—ã–≤–∞–µ–º Whisper.")
            text_final = self.run_whisper(waveform)
            logging.debug(f"üÜï Whisper –ø—Ä–µ–¥—Å–∫–∞–∑–∞–ª: {text_final}")
        else:
            if csv_text.strip():
                logging.debug("–¢–µ–∫—Å—Ç: –∏—Å–ø–æ–ª—å–∑—É–µ–º CSV-—Ç–µ–∫—Å—Ç (–Ω–µ –ø—É—Å—Ç).")
                text_final = csv_text
            else:
                if self.split == "train" or self.use_whisper_for_nontrain_if_no_text:
                    logging.debug("–¢–µ–∫—Å—Ç: CSV –ø—É—Å—Ç–æ–π ‚Äì –≤—ã–∑—ã–≤–∞–µ–º Whisper.")
                    text_final = self.run_whisper(waveform)
                else:
                    logging.debug("–¢–µ–∫—Å—Ç: CSV –ø—É—Å—Ç–æ–π –∏ –Ω–µ –≤—ã–∑—ã–≤–∞–µ–º Whisper –¥–ª—è dev/test.")
                    text_final = ""

        return {
            "audio_path": os.path.basename(audio_path), # new
            "audio": waveform,
            "label": label_vec,
            "text": text_final
        }

    def load_audio(self, path):
        """
        –ó–∞–≥—Ä—É–∂–∞–µ—Ç –∞—É–¥–∏–æ –ø–æ —É–∫–∞–∑–∞–Ω–Ω–æ–º—É –ø—É—Ç–∏ –∏ —Ä–µ—Å—ç–º–ø–ª–∏—Ä—É–µ—Ç –µ–≥–æ –¥–æ self.sample_rate, –µ—Å–ª–∏ –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ.
        """
        if not os.path.exists(path):
            logging.warning(f"–§–∞–π–ª –æ—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç: {path}")
            return None, None
        try:
            wf, sr = torchaudio.load(path)
            if sr != self.sample_rate:
                resampler = torchaudio.transforms.Resample(sr, self.sample_rate)
                wf = resampler(wf)
                sr = self.sample_rate
            return wf, sr
        except Exception as e:
            logging.error(f"–û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ {path}: {e}")
            return None, None

    def get_suitable_audio(self, label_name, exclude_path, min_needed, top_k=5):
        """
        –ò—â–µ—Ç –∞—É–¥–∏–æ—Ñ–∞–π–ª —Å —Ç–æ–π –∂–µ —ç–º–æ—Ü–∏–µ–π.
        1) –ï—Å–ª–∏ –µ—Å—Ç—å —Ñ–∞–π–ª—ã >= min_needed, –≤—ã–±–∏—Ä–∞–µ–º —Å–ª—É—á–∞–π–Ω–æ –∏–∑ –Ω–∏—Ö.
        2) –ï—Å–ª–∏ —Ç–∞–∫–∏—Ö –Ω–µ—Ç, –±–µ—Ä—ë–º —Ç–æ–ø-K —Å–∞–º—ã—Ö –¥–ª–∏–Ω–Ω—ã—Ö, –ø–æ—Ç–æ–º –∏–∑ –Ω–∏—Ö –±–µ—Ä—ë–º —Å–ª—É—á–∞–π–Ω—ã–π.
        """

        candidates = [p for p, lbl in self.audio_class_map.items()
                    if lbl == label_name and p != exclude_path]
        logging.debug(f"üîç –ù–∞–π–¥–µ–Ω–æ {len(candidates)} –∫–∞–Ω–¥–∏–¥–∞—Ç–æ–≤ –¥–ª—è —ç–º–æ—Ü–∏–∏ '{label_name}'")

        # –°–æ—Ö—Ä–∞–Ω–∏–º: (eq_len, path) –¥–ª—è –≤—Å–µ—Ö –∫–∞–Ω–¥–∏–¥–∞—Ç–æ–≤, –Ω–æ –ë–ï–ó –ø–æ–≤—Ç–æ—Ä–Ω–æ–≥–æ —á—Ç–µ–Ω–∏—è torchaudio.info
        all_info = []
        for path in candidates:
            # <<< NEW: –≤–º–µ—Å—Ç–æ info = torchaudio.info(path) ...
            eq_len = self.path_info.get(path, 0)  # –ü–æ–ª—É—á–∞–µ–º –∏–∑ –∫—ç—à–∞
            all_info.append((eq_len, path))

        # --- –ù–∏–∂–µ —Å—Ç–∞—Ä—ã–π –∫–æ–¥, –∫–æ—Ç–æ—Ä—ã–π –±—ã–ª:
        # for path in candidates:
        #     try:
        #         info = torchaudio.info(path)
        #         length = info.num_frames
        #         sr_ = info.sample_rate
        #         eq_len = int(length / (sr_ / self.sample_rate)) if sr_ != self.sample_rate else length
        #         all_info.append((eq_len, path))
        #     except Exception as e:
        #         logging.warning(f"‚ö† –û—à–∏–±–∫–∞ —á—Ç–µ–Ω–∏—è {path}: {e}")

        # 1) –§–∏–ª—å—Ç—Ä—É–µ–º —Ç–æ–ª—å–∫–æ >= min_needed
        valid = [(l, p) for l, p in all_info if l >= min_needed]
        logging.debug(f"‚úÖ –ü–æ–¥—Ö–æ–¥—è—â–∏—Ö (>= {min_needed}): {len(valid)} (–∏–∑ {len(all_info)})")

        if valid:
            # –ï—Å–ª–∏ –µ—Å—Ç—å –∏–¥–µ–∞–ª—å–Ω—ã–µ ‚Äî –±–µ—Ä—ë–º —Å–ª—É—á–∞–π–Ω–æ –∏–∑ –Ω–∏—Ö
            random.shuffle(valid)
            chosen = random.choice(valid)[1]
            return chosen
        else:
            # 2) –ï—Å–ª–∏ –∏–¥–µ–∞–ª—å–Ω—ã—Ö –Ω–µ—Ç ‚Äî –±–µ—Ä—ë–º —Ç–æ–ø-K –ø–æ –¥–ª–∏–Ω–µ
            sorted_by_len = sorted(all_info, key=lambda x: x[0], reverse=True)
            top_k_list = sorted_by_len[:top_k]
            if not top_k_list:
                logging.debug("–ù–µ—Ç –¥–æ—Å—Ç—É–ø–Ω—ã—Ö –∫–∞–Ω–¥–∏–¥–∞—Ç–æ–≤ –≤–æ–æ–±—â–µ.")
                return None  # –≤–æ–æ–±—â–µ –Ω–µ—Ç –∫–∞–Ω–¥–∏–¥–∞—Ç–æ–≤

            random.shuffle(top_k_list)
            chosen = top_k_list[0][1]
            logging.info(f"–ò–∑ —Ç–æ–ø-{top_k} –≤—ã–±—Ä–∞–Ω –∫–∞–Ω–¥–∏–¥–∞—Ç: {chosen}")
            return chosen

    def run_whisper(self, waveform):
        """
        –í—ã–∑—ã–≤–∞–µ—Ç Whisper –Ω–∞ –∞—É–¥–∏–æ—Å–∏–≥–Ω–∞–ª–µ –∏ –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç –ø–æ–ª–Ω—ã–π —Ç–µ–∫—Å—Ç (–±–µ–∑ –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏—è –ø–æ –∫–æ–ª–∏—á–µ—Å—Ç–≤—É —Å–ª–æ–≤).
        """
        arr = waveform.squeeze().cpu().numpy()
        try:
            result = self.whisper_model.transcribe(arr, fp16=False)
            text = result["text"].strip()
            return text
        except Exception as e:
            logging.error(f"Whisper –æ—à–∏–±–∫–∞: {e}")
            return ""

    def emotion_to_vector(self, label_name):
        """
        –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ—Ç –Ω–∞–∑–≤–∞–Ω–∏–µ —ç–º–æ—Ü–∏–∏ –≤ one-hot –≤–µ–∫—Ç–æ—Ä (torch.tensor).
        """
        v = np.zeros(len(self.emotion_columns), dtype=np.float32)
        if label_name in self.emotion_columns:
            idx = self.emotion_columns.index(label_name)
            v[idx] = 1.0
        return torch.tensor(v, dtype=torch.float32)
