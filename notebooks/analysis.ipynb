{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1e514e1-c921-4fdb-a877-fef7a22d73cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "\n",
    "# ───────────────────────── Парсер ──────────────────────────\n",
    "def parse_smart_log(path_or_str, top_n=15):\n",
    "    \"\"\"Принимает путь к .txt‑файлу или сам текст лога, \n",
    "       отдаёт DataFrame со шагами, dev/test/gap и всеми гиперпараметрами.\"\"\"\n",
    "    \n",
    "    # читаем либо из файла, либо из уже переданной строки\n",
    "    if '\\n' in path_or_str or 'Шаг' in path_or_str:\n",
    "        lines = path_or_str.splitlines()\n",
    "    else:\n",
    "        with open(path_or_str, encoding='utf-8') as f:\n",
    "            lines = f.readlines()\n",
    "\n",
    "    rows, current = [], {}\n",
    "\n",
    "    step_re = re.compile(\n",
    "        r\"Шаг\\s+(\\d+):\\s*([^=]+?)=\\s*\\((.*?)\\)\"\n",
    "    )\n",
    "    mean_re = re.compile(r\"MEAN\\s*=\\s*([0-9.]+)\")\n",
    "    gap_re  = re.compile(r\"GAP\\s*=\\s*([+-]?[0-9.]+)\")\n",
    "\n",
    "    for i, raw in enumerate(lines):\n",
    "        line = raw.rstrip(\"\\n\")\n",
    "        \n",
    "        # ── 1. ищем строку «Шаг N: …»  ───────────────────\n",
    "        m = step_re.search(line)\n",
    "        if m:\n",
    "            # если предыдущий step уже набрал все метрики — сохраняем\n",
    "            if current.get('dev') and current.get('test'):\n",
    "                current.setdefault('gap', round(current['test'] - current['dev'], 4))\n",
    "                rows.append(current)\n",
    "            # начинаем новый шаг\n",
    "            current = {'step': int(m.group(1))}\n",
    "            \n",
    "            keys = [k.strip() for k in m.group(2).split('+')]\n",
    "            raw_vals = re.findall(r\"'[^']*'|[^,]+\", m.group(3))\n",
    "            vals = [v.strip().strip(\"'\") for v in raw_vals]\n",
    "            for k, v in zip(keys, vals):\n",
    "                try:\n",
    "                    current[k] = eval(v)      # превращаем 0.001 → float, 8 → int\n",
    "                except Exception:\n",
    "                    current[k] = v            # если это строка без кавычек\n",
    "    \n",
    "        # ── 2. «Результаты (DEV):»  ───────────────────────\n",
    "        if \"Результаты (DEV):\" in line:\n",
    "            for j in range(i + 1, len(lines)):\n",
    "                m = mean_re.search(lines[j])\n",
    "                if m:\n",
    "                    current['dev'] = float(m.group(1))\n",
    "                    break\n",
    "        \n",
    "        # ── 3. «Результаты (TEST):» + GAP  ────────────────\n",
    "        if \"Результаты (TEST):\" in line:\n",
    "            for j in range(i + 1, len(lines)):\n",
    "                m = mean_re.search(lines[j])\n",
    "                if m:\n",
    "                    current['test'] = float(m.group(1))\n",
    "                    break\n",
    "            for j in range(i + 1, len(lines)):\n",
    "                g = gap_re.search(lines[j])\n",
    "                if g:\n",
    "                    current['gap'] = float(g.group(1))\n",
    "                    break\n",
    "\n",
    "    # не забываем «добавить хвост»\n",
    "    if current.get('dev') and current.get('test'):\n",
    "        current.setdefault('gap', round(current['test'] - current['dev'], 4))\n",
    "        rows.append(current)\n",
    "\n",
    "    df = pd.DataFrame(rows)\n",
    "    if not df.empty:\n",
    "        df = df.sort_values('test', ascending=False)\n",
    "        if top_n is not None:\n",
    "            df = df.head(top_n)\n",
    "        df = df.reset_index(drop=True)\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d40405fe-1159-4d73-94ff-1084124840a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = parse_smart_log(\"C:/Users/Alexandr/Desktop/sampling/10.txt\",25)\n",
    "\n",
    "from IPython.display import display\n",
    "pd.set_option(\"display.max_columns\", None)\n",
    "pd.set_option(\"display.width\", 160)\n",
    "\n",
    "display(df.head(15))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "248d2c65-2222-44b5-a83e-20e1c2048ba4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = parse_smart_log(\"C:/Users/Alexandr/Desktop/sampling/20.txt\",25)\n",
    "\n",
    "from IPython.display import display\n",
    "pd.set_option(\"display.max_columns\", None)\n",
    "pd.set_option(\"display.width\", 160)\n",
    "\n",
    "display(df.head(15))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e06dfea-d6cc-479b-8113-3b0140840db8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = parse_smart_log(\"C:/Users/Alexandr/Desktop/sampling/30.txt\",25)\n",
    "\n",
    "from IPython.display import display\n",
    "pd.set_option(\"display.max_columns\", None)\n",
    "pd.set_option(\"display.width\", 160)\n",
    "\n",
    "display(df.head(15))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3f7929b-5279-4490-84e6-f0e4309f769d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = parse_smart_log(\"C:/Users/Alexandr/Desktop/sampling/40.txt\",25)\n",
    "\n",
    "from IPython.display import display\n",
    "pd.set_option(\"display.max_columns\", None)\n",
    "pd.set_option(\"display.width\", 160)\n",
    "\n",
    "display(df.head(15))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70da89eb-18e6-4795-8b83-1116fe1fa968",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = parse_smart_log(\"C:/Users/Alexandr/Desktop/sampling/50.txt\",25)\n",
    "\n",
    "from IPython.display import display\n",
    "pd.set_option(\"display.max_columns\", None)\n",
    "pd.set_option(\"display.width\", 160)\n",
    "\n",
    "display(df.head(15))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d510c0d0-640d-4647-932a-a05847c5d56b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f2c7085-3402-4bf6-9c18-b6fc24c01e78",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2b8374d-b115-4a8a-83a1-b021897307e3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85057aaf-62ae-43d9-8bea-c0f860bef2e7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4916d4f-b2d7-4f22-b6f9-ffb27d7930de",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "138d9ac5-ccad-4249-b989-6d53f574879d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cc86b01-9e50-4fc5-b67e-5004460d2cce",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6614a37d-344e-46a5-b5ba-e49d4010027a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = parse_smart_log(\"C:/Users/Alexandr/Desktop/sampling/60.txt\",25)\n",
    "\n",
    "from IPython.display import display\n",
    "pd.set_option(\"display.max_columns\", None)\n",
    "pd.set_option(\"display.width\", 160)\n",
    "\n",
    "display(df.head(25))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f6b722c-c134-45ec-9cf0-b4b4f8eb0c3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = parse_smart_log(\"C:/Users/Alexandr/Desktop/sampling/70.txt\",25)\n",
    "\n",
    "from IPython.display import display\n",
    "pd.set_option(\"display.max_columns\", None)\n",
    "pd.set_option(\"display.width\", 160)\n",
    "\n",
    "display(df.head(25))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d4db04e-ee0f-4c2c-b0d2-45edbf2128dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = parse_smart_log(\"C:/Users/Alexandr/Desktop/sampling/80.txt\",25)\n",
    "\n",
    "from IPython.display import display\n",
    "pd.set_option(\"display.max_columns\", None)\n",
    "pd.set_option(\"display.width\", 160)\n",
    "\n",
    "display(df.head(25))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff64fbf5-cea1-4ee1-b7dc-d415ed7de9e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = parse_smart_log(\"C:/Users/Alexandr/Desktop/sampling/90.txt\",25)\n",
    "\n",
    "from IPython.display import display\n",
    "pd.set_option(\"display.max_columns\", None)\n",
    "pd.set_option(\"display.width\", 160)\n",
    "\n",
    "display(df.head(25))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00b62770-f38f-405c-9c5c-630d4afd7d26",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = parse_smart_log(\"C:/Users/Alexandr/Desktop/sampling/100.txt\",25)\n",
    "\n",
    "from IPython.display import display\n",
    "pd.set_option(\"display.max_columns\", None)\n",
    "pd.set_option(\"display.width\", 160)\n",
    "\n",
    "display(df.head(25))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb9186a9-f0bc-406d-b2c3-724d7b5f9d43",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f9fbc1c-ab41-4dca-bbad-6a0eadd28f7f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75806125-04ac-4e18-968e-4632b92d1d16",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ac6d74a-3625-4c46-b1a1-7d70a7ef5446",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99e64f6f-eeb4-4f1f-9bd0-b2cb230dc1da",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "467b04df-a408-4808-941d-5b0f2ebbf217",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = parse_smart_log(\"C:/Users/Alexandr/Desktop/sampling/bi/10.txt\",25)\n",
    "\n",
    "from IPython.display import display\n",
    "pd.set_option(\"display.max_columns\", None)\n",
    "pd.set_option(\"display.width\", 160)\n",
    "\n",
    "display(df.head(25))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efba3e34-f64b-4962-9c8b-ef70294badb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = parse_smart_log(\"C:/Users/Alexandr/Desktop/sampling/bi/20.txt\",25)\n",
    "\n",
    "from IPython.display import display\n",
    "pd.set_option(\"display.max_columns\", None)\n",
    "pd.set_option(\"display.width\", 160)\n",
    "\n",
    "display(df.head(25))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c48bc72-cac8-4119-96ad-a1f09cfed996",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = parse_smart_log(\"C:/Users/Alexandr/Desktop/sampling/bi/30.txt\",25)\n",
    "\n",
    "from IPython.display import display\n",
    "pd.set_option(\"display.max_columns\", None)\n",
    "pd.set_option(\"display.width\", 160)\n",
    "\n",
    "display(df.head(25))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a93f7c97-3e3c-4e6f-8bcb-454480aae5f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = parse_smart_log(\"C:/Users/Alexandr/Desktop/sampling/bi/40.txt\",25)\n",
    "\n",
    "from IPython.display import display\n",
    "pd.set_option(\"display.max_columns\", None)\n",
    "pd.set_option(\"display.width\", 160)\n",
    "\n",
    "display(df.head(25))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a488eb61-89a5-4c79-82a6-3980a5621fdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = parse_smart_log(\"C:/Users/Alexandr/Desktop/sampling/bi/50.txt\",25)\n",
    "\n",
    "from IPython.display import display\n",
    "pd.set_option(\"display.max_columns\", None)\n",
    "pd.set_option(\"display.width\", 160)\n",
    "\n",
    "display(df.head(25))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e7603d5-f30b-4304-b6b6-a59daf1f5356",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = parse_smart_log(\"C:/Users/Alexandr/Desktop/sampling/5862_адам лучший.txt\",25)\n",
    "\n",
    "from IPython.display import display\n",
    "pd.set_option(\"display.max_columns\", None)\n",
    "pd.set_option(\"display.width\", 160)\n",
    "\n",
    "display(df.head(25))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22d74f6b-4d7f-4a1a-a98b-6754c286712d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = parse_smart_log(\"C:/Users/Alexandr/Desktop/sampling/sgd_2.txt\",25)\n",
    "\n",
    "from IPython.display import display\n",
    "pd.set_option(\"display.max_columns\", None)\n",
    "pd.set_option(\"display.width\", 160)\n",
    "\n",
    "display(df.head(25))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35ae96f7-7ceb-46da-9411-ceba258a98ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = parse_smart_log(\"C:/Users/Alexandr/Desktop/sampling/last/biformer.txt\",50)\n",
    "\n",
    "from IPython.display import display\n",
    "pd.set_option(\"display.max_columns\", None)\n",
    "pd.set_option(\"display.width\", 160)\n",
    "\n",
    "display(df.head(50))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5832fe53-8308-4195-bb94-db7a86148076",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df = parse_smart_log(\"C:/Users/Alexandr/Desktop/sampling/last/BiForm_wtb.txt\",50)\n",
    "\n",
    "from IPython.display import display\n",
    "pd.set_option(\"display.max_columns\", None)\n",
    "pd.set_option(\"display.width\", 160)\n",
    "\n",
    "display(df.head(50))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d46eb44-6c34-4cc9-8322-6eb476eb827c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df = parse_smart_log(\"C:/Users/Alexandr/Desktop/sampling/smoothing.txt\",50)\n",
    "\n",
    "from IPython.display import display\n",
    "pd.set_option(\"display.max_columns\", None)\n",
    "pd.set_option(\"display.width\", 160)\n",
    "\n",
    "display(df.head(50))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b4e6f8c-4428-4c5c-a8ac-b1780372ee98",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df = parse_smart_log(\"C:/Users/Alexandr/Desktop/sampling/last/mambas.txt\",50)\n",
    "\n",
    "from IPython.display import display\n",
    "pd.set_option(\"display.max_columns\", None)\n",
    "pd.set_option(\"display.width\", 160)\n",
    "\n",
    "display(df.head(50))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45434e0f-2af9-491a-bfc9-54cc3a91fca9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = parse_smart_log(\"C:/Users/Alexandr/Desktop/sampling/last/bigated.txt\",50)\n",
    "\n",
    "from IPython.display import display\n",
    "pd.set_option(\"display.max_columns\", None)\n",
    "pd.set_option(\"display.width\", 160)\n",
    "\n",
    "display(df.head(50))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "041a82f6-a5db-4033-8e89-f4ecc24479f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = parse_smart_log(\"C:/Users/Alexandr/Desktop/sampling/last/bigraph.txt\",50)\n",
    "\n",
    "from IPython.display import display\n",
    "pd.set_option(\"display.max_columns\", None)\n",
    "pd.set_option(\"display.width\", 160)\n",
    "\n",
    "display(df.head(50))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42ebb696-9e6d-4ba1-acd4-b8802ecf0fac",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df = parse_smart_log(\"C:/Users/Alexandr/Desktop/sampling/last/bigatedgraph.txt\",50)\n",
    "\n",
    "from IPython.display import display\n",
    "pd.set_option(\"display.max_columns\", None)\n",
    "pd.set_option(\"display.width\", 160)\n",
    "\n",
    "display(df.head(50))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfae755b-6a58-4c26-8c83-4884038b321e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = parse_smart_log(\"C:/Users/Alexandr/Desktop/sampling/last/smothing/phi.txt\",50)\n",
    "\n",
    "from IPython.display import display\n",
    "pd.set_option(\"display.max_columns\", None)\n",
    "pd.set_option(\"display.width\", 160)\n",
    "\n",
    "display(df.head(50))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4db3748-6be0-4ee6-abd1-c0e734efc8c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = parse_smart_log(\"C:/Users/Alexandr/Desktop/sampling/last/smothing/qwen.txt\",50)\n",
    "\n",
    "from IPython.display import display\n",
    "pd.set_option(\"display.max_columns\", None)\n",
    "pd.set_option(\"display.width\", 160)\n",
    "\n",
    "display(df.head(50))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "952affd8-2fea-481c-b8cc-3f53418b5472",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = parse_smart_log(\"C:/Users/Alexandr/Desktop/sampling/last/BiGraphFormerWithProb.txt\",50)\n",
    "\n",
    "from IPython.display import display\n",
    "pd.set_option(\"display.max_columns\", None)\n",
    "pd.set_option(\"display.width\", 160)\n",
    "\n",
    "display(df.head(50))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c210111f-e1e1-4276-8e47-249bda79b189",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = parse_smart_log(\"C:/Users/Alexandr/Desktop/sampling/last/BiGatedGraphFormerWithProb.txt\",50)\n",
    "\n",
    "from IPython.display import display\n",
    "pd.set_option(\"display.max_columns\", None)\n",
    "pd.set_option(\"display.width\", 160)\n",
    "\n",
    "display(df.head(50))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2e06ea3-d6ba-48d4-a013-15b1145d9a4a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f9a4eae-c094-40c8-a0ee-ecf7c884d523",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8d39259-0192-4df1-8eef-531dee7f45b8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40afd60f-78b2-4b26-b5e2-314dddfa6c3f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c90849c-69d8-484e-b6a4-96a8e6447bc7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3325846-ffbc-444f-8549-2fbe2f5d6fde",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03aaa192-92c8-4938-bc18-8eac086e4648",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d32b1b7f-bcb6-45bd-930c-f93b4b40b3f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ======================================================================\n",
    "# Проверка синтетического корпуса MELD-S на «битые» эмбеддинги\n",
    "# ======================================================================\n",
    "\n",
    "# ---------- 1. Импорты и базовые настройки ----------\n",
    "import os, logging, traceback\n",
    "import torch, torchaudio\n",
    "import pandas as pd\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# --- если проект находится в другом каталоге, добавьте его в sys.path ---\n",
    "# import sys; sys.path.append(r\"C:\\Prgrm\\ESWA_2025\")\n",
    "\n",
    "from data_loading.feature_extractor import (\n",
    "    PretrainedAudioEmbeddingExtractor,\n",
    "    PretrainedTextEmbeddingExtractor,\n",
    ")\n",
    "\n",
    "# ---------- 2. Пути из вашего config.toml ----------\n",
    "synthetic_path = r\"E:/MELD_S\"\n",
    "synth_csv_path = os.path.join(synthetic_path, \"meld_s_train_labels.csv\")\n",
    "synth_wav_dir  = os.path.join(synthetic_path, \"wavs\")\n",
    "\n",
    "# ---------- 3. Создаём экстракторы ровно как в основном проекте ----------\n",
    "AUDIO_MODEL     = \"audeering/wav2vec2-large-robust-12-ft-emotion-msp-dim\"\n",
    "AUDIO_CKPT      = \"best_audio_model_2.pt\"          # путь относительно запуска\n",
    "TEXT_MODEL      = \"jinaai/jina-embeddings-v3\"\n",
    "TEXT_CKPT       = \"best_text_model.pth\"\n",
    "DEVICE          = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "SAMPLE_RATE     = 16000\n",
    "\n",
    "audio_feat = PretrainedAudioEmbeddingExtractor(\n",
    "    model_name=AUDIO_MODEL,\n",
    "    checkpoint=AUDIO_CKPT,\n",
    "    device=DEVICE,\n",
    ")\n",
    "\n",
    "text_feat = PretrainedTextEmbeddingExtractor(\n",
    "    model_name=TEXT_MODEL,\n",
    "    checkpoint=TEXT_CKPT,\n",
    "    device=DEVICE,\n",
    ")\n",
    "\n",
    "# ---------- 4. Узнаём фактические размеры эмбеддингов ----------\n",
    "with torch.no_grad():\n",
    "    dummy_wav = torch.zeros(1, SAMPLE_RATE)            # секунда тишины\n",
    "    _, a_emb = audio_feat.extract(dummy_wav[0], SAMPLE_RATE)\n",
    "    AUDIO_DIM = a_emb[0].shape[-1]\n",
    "\n",
    "    _, t_emb = text_feat.extract(\"hello world\")\n",
    "    TEXT_DIM  = t_emb[0].shape[-1]\n",
    "\n",
    "# сколько логитов выдаёт каждый классификатор\n",
    "NUM_EMOTIONS = 7          # [\"anger\", \"disgust\", ...] — как в config\n",
    "PRED_DIM = NUM_EMOTIONS\n",
    "\n",
    "EXPECTED_ALL = AUDIO_DIM + TEXT_DIM + 2 * PRED_DIM\n",
    "print(f\"AUDIO_DIM = {AUDIO_DIM},  TEXT_DIM = {TEXT_DIM},  \"\n",
    "      f\"TOTAL EXPECTED = {EXPECTED_ALL}\")\n",
    "\n",
    "# ---------- 5. Читаем CSV синтетики ----------\n",
    "df = pd.read_csv(synth_csv_path)\n",
    "print(f\"Всего строк в CSV: {len(df)}\")\n",
    "\n",
    "bad_rows, good_cnt = [], 0\n",
    "\n",
    "# ---------- 6. Проходим по записям ----------\n",
    "for i, row in tqdm(df.iterrows(), total=len(df)):\n",
    "    video_name = row[\"video_name\"]\n",
    "    wav_path   = os.path.join(synth_wav_dir, f\"{video_name}.wav\")\n",
    "    txt        = row.get(\"text\", \"\")\n",
    "\n",
    "    reason = None\n",
    "    try:\n",
    "        # 6.1 Проверяем, существует ли wav-файл\n",
    "        if not os.path.exists(wav_path):\n",
    "            reason = \"file_missing\"\n",
    "\n",
    "        # 6.2 Получаем аудио-эмбеддинг\n",
    "        if reason is None:\n",
    "            wf, sr = torchaudio.load(wav_path)\n",
    "            if sr != SAMPLE_RATE:\n",
    "                wf = torchaudio.transforms.Resample(sr, SAMPLE_RATE)(wf)\n",
    "            a_pred, a_emb = audio_feat.extract(wf[0], SAMPLE_RATE)\n",
    "            a_emb = a_emb[0]\n",
    "            if a_emb.shape[-1] != AUDIO_DIM:\n",
    "                reason = f\"audio_dim_{a_emb.shape[-1]}\"\n",
    "\n",
    "        # 6.3 Получаем текст-эмбеддинг\n",
    "        if reason is None:\n",
    "            t_pred, t_emb = text_feat.extract(txt)\n",
    "            t_emb = t_emb[0]\n",
    "            if t_emb.shape[-1] != TEXT_DIM:\n",
    "                reason = f\"text_dim_{t_emb.shape[-1]}\"\n",
    "\n",
    "        # 6.4 Проверяем полную конкатенацию\n",
    "        if reason is None:\n",
    "            full_vec = torch.cat([a_emb, t_emb, a_pred[0], t_pred[0]], dim=-1)\n",
    "            if full_vec.shape[-1] != EXPECTED_ALL:\n",
    "                reason = f\"concat_dim_{full_vec.shape[-1]}\"\n",
    "\n",
    "    except Exception as e:\n",
    "        reason = \"exception_\" + e.__class__.__name__\n",
    "        logging.error(f\"{video_name}: {traceback.format_exc(limit=2)}\")\n",
    "\n",
    "    # 6.5 Сохраняем результат\n",
    "    if reason:\n",
    "        bad_rows.append({\n",
    "            \"idx\": i,\n",
    "            \"video_name\": video_name,\n",
    "            \"reason\": reason,\n",
    "            \"wav_path\": wav_path,\n",
    "            \"text_len\": len(txt),\n",
    "        })\n",
    "    else:\n",
    "        good_cnt += 1\n",
    "\n",
    "# ---------- 7. Итоги ----------\n",
    "print(f\"\\n✅ GOOD : {good_cnt}\")\n",
    "print(f\"❌ BAD  : {len(bad_rows)}\")\n",
    "\n",
    "bad_df = pd.DataFrame(bad_rows)\n",
    "display(bad_df)\n",
    "\n",
    "# ---------- 8. (Необязательно) сохраняем список плохих файлов ----------\n",
    "out_csv = os.path.join(synthetic_path, \"bad_synth_meld.csv\")\n",
    "bad_df.to_csv(out_csv, index=False)\n",
    "print(f\"\\nСписок «битых» примеров сохранён в: {out_csv}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a232f93d-7f7c-41d3-9204-74445e43d071",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from models.models import BiFormer\n",
    "\n",
    "# Создание модели с параметрами из config.toml\n",
    "model = BiFormer(\n",
    "    audio_dim=256,\n",
    "    text_dim=1024,\n",
    "    seg_len=95,\n",
    "    hidden_dim=256,\n",
    "    hidden_dim_gated=256,\n",
    "    num_transformer_heads=8,\n",
    "    num_graph_heads=2,\n",
    "    positional_encoding=False,\n",
    "    dropout=0.15,\n",
    "    mode='mean',\n",
    "    device=\"cuda\",\n",
    "    tr_layer_number=5,\n",
    "    out_features=256,\n",
    "    num_classes=7\n",
    ")\n",
    "\n",
    "# Подсчёт параметров\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f\"Total parameters: {total_params:,}\")\n",
    "print(f\"Trainable parameters: {trainable_params:,}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89bdfe1b-6e1b-4224-b69f-a3149edbb4d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModel, Wav2Vec2Model, WhisperModel\n",
    "from transformers import AutoConfig\n",
    "from pathlib import Path\n",
    "import torch\n",
    "import os\n",
    "\n",
    "def count_parameters(model):\n",
    "    total_params = sum(p.numel() for p in model.parameters())\n",
    "    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    return total_params, trainable_params\n",
    "\n",
    "def get_model_size(model_path):\n",
    "    total_size = 0\n",
    "    for file in Path(model_path).rglob(\"*\"):\n",
    "        if file.is_file():\n",
    "            total_size += file.stat().st_size\n",
    "    return total_size / (1024 ** 2)  # MB\n",
    "\n",
    "# Список моделей\n",
    "models_info = {\n",
    "    \"Jina Embeddings V3\": (\"jinaai/jina-embeddings-v3\", AutoModel),\n",
    "    \"Wav2Vec2 Large Robust\": (\"audeering/wav2vec2-large-robust-12-ft-emotion-msp-dim\", Wav2Vec2Model),\n",
    "    \"Whisper Base\": (\"openai/whisper-base\", WhisperModel)\n",
    "}\n",
    "\n",
    "cache_dir = \"./hf_models\"  # Папка, куда будет загружаться\n",
    "\n",
    "for name, (model_name, model_class) in models_info.items():\n",
    "    print(f\"\\n📦 Загрузка: {name}\")\n",
    "    model = model_class.from_pretrained(model_name, cache_dir=cache_dir, trust_remote_code=True)\n",
    "    config = AutoConfig.from_pretrained(model_name, cache_dir=cache_dir, trust_remote_code=True)\n",
    "\n",
    "    total, trainable = count_parameters(model)\n",
    "    size_mb = get_model_size(Path(cache_dir) / model_name.replace(\"/\", \"-\"))\n",
    "\n",
    "    print(f\"🔹 {name}\")\n",
    "    print(f\"   • Total parameters:     {total:,}\")\n",
    "    print(f\"   • Trainable parameters: {trainable:,}\")\n",
    "    print(f\"   • Disk size:            {size_mb:.2f} MB\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bc322d7-ef0d-41eb-abc1-895ac5d176e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "from data_loading.pretrained_extractors import get_model_mamba, Mamba\n",
    "\n",
    "# Путь к весам\n",
    "AUDIO_PATH = \"best_audio_model_2.pt\"\n",
    "TEXT_PATH = \"best_text_model.pth\"\n",
    "\n",
    "# ===== Audio classifier =====\n",
    "audio_params = {\n",
    "    \"input_size\": 1024,\n",
    "    \"d_model\": 256,\n",
    "    \"num_layers\": 2,\n",
    "    \"num_classes\": 7,\n",
    "    \"dropout\": 0.2\n",
    "}\n",
    "audio_model = get_model_mamba(audio_params)\n",
    "audio_model.load_state_dict(torch.load(AUDIO_PATH, map_location=\"cpu\"))\n",
    "audio_params_count = sum(p.numel() for p in audio_model.parameters())\n",
    "\n",
    "# ===== Text classifier =====\n",
    "ckpt = torch.load(TEXT_PATH, map_location=\"cpu\")\n",
    "text_model = Mamba(\n",
    "    num_layers=2,\n",
    "    d_input=1024,\n",
    "    d_model=512,\n",
    "    num_classes=7,\n",
    "    model_name=\"jinaai/jina-embeddings-v3\",\n",
    "    max_tokens=128,\n",
    "    pooling=None\n",
    ")\n",
    "text_model.load_state_dict(ckpt[\"model_state_dict\"])\n",
    "text_params_count = sum(p.numel() for p in text_model.parameters())\n",
    "\n",
    "\n",
    "# ===== Disk size =====\n",
    "audio_size_mb = os.path.getsize(AUDIO_PATH) / 1024**2\n",
    "text_size_mb = os.path.getsize(TEXT_PATH) / 1024**2\n",
    "\n",
    "# ===== Print summary =====\n",
    "print(f\"🎙 Audio classifier: {audio_params_count:,} params | {audio_size_mb:.2f} MB\")\n",
    "print(f\"📝 Text classifier: {text_params_count:,} params | {text_size_mb:.2f} MB\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df9d8673-641c-4843-b466-88b50ce7c4f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from models.models import BiFormer\n",
    "from data_loading.feature_extractor import PretrainedAudioEmbeddingExtractor, PretrainedTextEmbeddingExtractor\n",
    "import torchaudio\n",
    "\n",
    "# === 1. Настройки\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "text = \"Why do all you’re coffee mugs have numbers on the bottom?\"\n",
    "audio_path = \"E:/MELD/wavs/test/dia0_utt0.wav\"\n",
    "\n",
    "# === 2. Загружаем аудио\n",
    "waveform, sr = torchaudio.load(audio_path)\n",
    "waveform = waveform.squeeze(0)  # (T,)\n",
    "\n",
    "# === 3. Экстрактор аудио-эмбеддингов\n",
    "audio_extractor = PretrainedAudioEmbeddingExtractor(config=type('cfg', (), {\n",
    "    \"audio_model_name\": \"audeering/wav2vec2-large-robust-12-ft-emotion-msp-dim\",\n",
    "    \"emb_device\": device,\n",
    "    \"audio_pooling\": \"mean\",\n",
    "    \"emb_normalize\": True,\n",
    "    \"max_audio_frames\": 0,\n",
    "    \"audio_classifier_checkpoint\": \"best_audio_model_2.pt\"\n",
    "})())\n",
    "\n",
    "# === 4. Экстрактор текст-эмбеддингов\n",
    "text_extractor = PretrainedTextEmbeddingExtractor(config=type('cfg', (), {\n",
    "    \"text_model_name\": \"jinaai/jina-embeddings-v3\",\n",
    "    \"emb_device\": device,\n",
    "    \"text_pooling\": \"mean\",\n",
    "    \"emb_normalize\": True,\n",
    "    \"max_tokens\": 128,\n",
    "    \"text_classifier_checkpoint\": \"best_text_model.pth\"\n",
    "})())\n",
    "\n",
    "# === 5. Извлечение эмбеддингов\n",
    "_, audio_emb = audio_extractor.extract(waveform, sample_rate=sr)\n",
    "_, text_emb = text_extractor.extract(text)\n",
    "\n",
    "# === 6. Загрузка BiFormer\n",
    "model = BiFormer(\n",
    "    audio_dim=256,\n",
    "    text_dim=1024,\n",
    "    seg_len=95,\n",
    "    hidden_dim=256,\n",
    "    hidden_dim_gated=256,\n",
    "    num_transformer_heads=8,\n",
    "    num_graph_heads=2,\n",
    "    positional_encoding=False,\n",
    "    dropout=0.15,\n",
    "    mode='mean',\n",
    "    device=device,\n",
    "    tr_layer_number=5,\n",
    "    out_features=256,\n",
    "    num_classes=7\n",
    ").to(device)\n",
    "\n",
    "model.load_state_dict(torch.load(\"checkpoints/best_model_dev_0_5895_epoch_8.pt\", map_location=device))\n",
    "model.eval()\n",
    "\n",
    "# === 7. Подготовка входов и инференс\n",
    "audio_emb = audio_emb.unsqueeze(0).to(device)  # (1, 256)\n",
    "text_emb = text_emb.unsqueeze(0).to(device)    # (1, 1024)\n",
    "logits = model(audio_emb, text_emb)\n",
    "pred_class = torch.argmax(logits, dim=1).item()\n",
    "\n",
    "# === 8. Вывод\n",
    "print(f\"Predicted emotion class: {pred_class}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f3aabcb-2d77-438a-a508-cae0bed39ec4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
