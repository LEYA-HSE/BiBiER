{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "bbffcc6e-dbf7-44f9-8595-81f9cefc8d79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üçæ CSV –≥–æ—Ç–æ–≤: E:/with_transcripts/ADReSS-IS2020-data/ADReSS-IS2020-data/transcripts_asr_openai_whisper-large-v3-turbo/test_train/ADReSS_IS2020_labels.csv ‚Äî –∏ —Ç–µ–ø–µ—Ä—å —Å –∫–æ–ª–æ–Ω–∫–æ–π 'dataset', —á—Ç–æ–±—ã –∂–∏–∑–Ω—å –±—ã–ª–∞ –ª–æ–≥–∏—á–Ω–µ–µ, —á–µ–º —ç—Ç–æ—Ç –ø—Ä–æ–µ–∫—Ç.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import csv\n",
    "\n",
    "# –ö–æ—Ä–Ω–µ–≤–∞—è –ø–∞–ø–∫–∞\n",
    "input_root = \"E:/with_transcripts/ADReSS-IS2020-data/ADReSS-IS2020-data/transcripts_asr_openai_whisper-large-v3-turbo/test_train/\"  # <- –ø–æ–¥—Å—Ç–∞–≤—å –ø—É—Ç—å –∫ —Å–≤–æ–µ–π train/audio\n",
    "output_csv  = \"E:/with_transcripts/ADReSS-IS2020-data/ADReSS-IS2020-data/transcripts_asr_openai_whisper-large-v3-turbo/test_train/ADReSS_IS2020_labels.csv\"      # <- –∫—É–¥–∞ —Å–æ—Ö—Ä–∞–Ω—è—Ç—å CSV (–º–æ–∂–Ω–æ —Ä—è–¥–æ–º)\n",
    "\n",
    "dataset_name = \"ADReSS_IS2020\"  \n",
    "\n",
    "results = []\n",
    "\n",
    "for root, dirs, files in os.walk(input_root):\n",
    "    for file in files:\n",
    "        if file.endswith(\".json\"):\n",
    "            input_path = os.path.join(root, file)\n",
    "            json_filename = os.path.splitext(file)[0]\n",
    "            \n",
    "            # –≤–æ—Ç —Ç—É—Ç –≤—Å—ë –º–µ–Ω—è–µ—Ç—Å—è\n",
    "            class_name = os.path.relpath(root, input_root).replace(\"\\\\\", \"/\")\n",
    "\n",
    "            try:\n",
    "                with open(input_path, \"r\", encoding=\"utf-8\") as f:\n",
    "                    data = json.load(f)\n",
    "            except json.JSONDecodeError:\n",
    "                print(f\"ü§¢ –ì–Ω–∏–ª–æ–π —Ñ–∞–π–ª: {input_path}\")\n",
    "                continue\n",
    "\n",
    "            is_dialogue = False\n",
    "            dialog_parts = []\n",
    "\n",
    "            for entry in data:\n",
    "                speaker = entry.get(\"speaker\", \"unknown\")\n",
    "                words = [w[\"word\"] for w in entry.get(\"words\", [])]\n",
    "                if not words:\n",
    "                    continue\n",
    "                sentence = \" \".join(words).strip()\n",
    "\n",
    "                if speaker == \"unknown\":\n",
    "                    dialog_parts.append(f\"{sentence}.\")\n",
    "                else:\n",
    "                    is_dialogue = True\n",
    "                    dialog_parts.append(f\"{speaker}: {sentence}\")\n",
    "\n",
    "            full_text = \" \".join(dialog_parts) + \".\" if is_dialogue else \" \".join(dialog_parts)\n",
    "\n",
    "            results.append({\n",
    "                \"dataset\": dataset_name,\n",
    "                \"class\": class_name,  # <- —Ç–µ–ø–µ—Ä—å —ç—Ç–æ —á—Ç–æ-—Ç–æ —Ç–∏–ø–∞ test/cc\n",
    "                \"file\": json_filename,\n",
    "                \"sentence\": full_text\n",
    "            })\n",
    "            \n",
    "# –°–æ—Ö—Ä–∞–Ω—è–µ–º CSV\n",
    "os.makedirs(os.path.dirname(output_csv), exist_ok=True)\n",
    "\n",
    "with open(output_csv, \"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "    writer = csv.DictWriter(f, fieldnames=[\"dataset\", \"class\", \"file\", \"sentence\"])\n",
    "    writer.writeheader()\n",
    "    for row in results:\n",
    "        writer.writerow(row)\n",
    "\n",
    "print(f\"üçæ CSV –≥–æ—Ç–æ–≤: {output_csv} ‚Äî –∏ —Ç–µ–ø–µ—Ä—å —Å –∫–æ–ª–æ–Ω–∫–æ–π 'dataset', —á—Ç–æ–±—ã –∂–∏–∑–Ω—å –±—ã–ª–∞ –ª–æ–≥–∏—á–Ω–µ–µ, —á–µ–º —ç—Ç–æ—Ç –ø—Ä–æ–µ–∫—Ç.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c143153c-65ad-44ee-8ea6-d8a5a9933fba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üçæ CSV –≥–æ—Ç–æ–≤: E:/with_transcripts/DemoCare_struct/DemoCare_struct/transcripts_asr_openai_whisper-large-v3-turbo/DemoCare_labels.csv ‚Äî –∏ —Ç–µ–ø–µ—Ä—å —Å –∫–æ–ª–æ–Ω–∫–æ–π 'dataset', —á—Ç–æ–±—ã –∂–∏–∑–Ω—å –±—ã–ª–∞ –ª–æ–≥–∏—á–Ω–µ–µ, —á–µ–º —ç—Ç–æ—Ç –ø—Ä–æ–µ–∫—Ç.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import csv\n",
    "\n",
    "# –ö–æ—Ä–Ω–µ–≤–∞—è –ø–∞–ø–∫–∞\n",
    "input_root = \"E:/with_transcripts/DemoCare_struct/DemoCare_struct/transcripts_asr_openai_whisper-large-v3-turbo/\"  # <- –ø–æ–¥—Å—Ç–∞–≤—å –ø—É—Ç—å –∫ —Å–≤–æ–µ–π train/audio\n",
    "output_csv  = \"E:/with_transcripts/DemoCare_struct/DemoCare_struct/transcripts_asr_openai_whisper-large-v3-turbo/DemoCare_labels.csv\"      # <- –∫—É–¥–∞ —Å–æ—Ö—Ä–∞–Ω—è—Ç—å CSV (–º–æ–∂–Ω–æ —Ä—è–¥–æ–º)\n",
    "\n",
    "dataset_name = \"DemoCare\"  # <- —Ç—ã —ç—Ç–æ —Å–∞–º –º–µ–Ω—è–µ—à—å, –ø–æ–Ω—è–ª?\n",
    "\n",
    "results = []\n",
    "\n",
    "for root, dirs, files in os.walk(input_root):\n",
    "    for file in files:\n",
    "        if file.endswith(\".json\"):\n",
    "            input_path = os.path.join(root, file)\n",
    "            json_filename = os.path.splitext(file)[0]\n",
    "            class_name = os.path.basename(os.path.relpath(root, input_root))\n",
    "\n",
    "            try:\n",
    "                with open(input_path, \"r\", encoding=\"utf-8\") as f:\n",
    "                    data = json.load(f)\n",
    "            except json.JSONDecodeError:\n",
    "                print(f\"ü§¢ –ì–Ω–∏–ª–æ–π —Ñ–∞–π–ª: {input_path}\")\n",
    "                continue\n",
    "\n",
    "            is_dialogue = False\n",
    "            dialog_parts = []\n",
    "\n",
    "            for entry in data:\n",
    "                speaker = entry.get(\"speaker\", \"unknown\")\n",
    "                words = [w[\"word\"] for w in entry.get(\"words\", [])]\n",
    "                if not words:\n",
    "                    continue\n",
    "                sentence = \" \".join(words).strip()\n",
    "\n",
    "                if speaker == \"unknown\":\n",
    "                    dialog_parts.append(f\"{sentence}.\")\n",
    "                else:\n",
    "                    is_dialogue = True\n",
    "                    dialog_parts.append(f\"{speaker}: {sentence}\")\n",
    "\n",
    "            # –°–∫–ª–µ–π–∫–∞\n",
    "            if is_dialogue:\n",
    "                full_text = \" \".join(dialog_parts) + \".\"\n",
    "            else:\n",
    "                full_text = \" \".join(dialog_parts)\n",
    "\n",
    "            results.append({\n",
    "                \"dataset\": dataset_name,\n",
    "                \"class\": class_name,\n",
    "                \"file\": json_filename,\n",
    "                \"sentence\": full_text\n",
    "            })\n",
    "\n",
    "# –°–æ—Ö—Ä–∞–Ω—è–µ–º CSV\n",
    "os.makedirs(os.path.dirname(output_csv), exist_ok=True)\n",
    "\n",
    "with open(output_csv, \"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "    writer = csv.DictWriter(f, fieldnames=[\"dataset\", \"class\", \"file\", \"sentence\"])\n",
    "    writer.writeheader()\n",
    "    for row in results:\n",
    "        writer.writerow(row)\n",
    "\n",
    "print(f\"üçæ CSV –≥–æ—Ç–æ–≤: {output_csv} ‚Äî –∏ —Ç–µ–ø–µ—Ä—å —Å –∫–æ–ª–æ–Ω–∫–æ–π 'dataset', —á—Ç–æ–±—ã –∂–∏–∑–Ω—å –±—ã–ª–∞ –ª–æ–≥–∏—á–Ω–µ–µ, —á–µ–º —ç—Ç–æ—Ç –ø—Ä–æ–µ–∫—Ç.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "48d076f6-0789-4376-afe7-12f4b010858e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üçæ CSV –≥–æ—Ç–æ–≤: E:/with_transcripts/TAUKADIAL-24/TAUKADIAL-24/transcripts_asr_openai_whisper-large-v3-turbo/TAUKADIAL_24_labels.csv ‚Äî –∏ —Ç–µ–ø–µ—Ä—å —Å –∫–æ–ª–æ–Ω–∫–æ–π 'dataset', —á—Ç–æ–±—ã –∂–∏–∑–Ω—å –±—ã–ª–∞ –ª–æ–≥–∏—á–Ω–µ–µ, —á–µ–º —ç—Ç–æ—Ç –ø—Ä–æ–µ–∫—Ç.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import csv\n",
    "\n",
    "# –ö–æ—Ä–Ω–µ–≤–∞—è –ø–∞–ø–∫–∞\n",
    "input_root = \"E:/with_transcripts/TAUKADIAL-24/TAUKADIAL-24/transcripts_asr_openai_whisper-large-v3-turbo/\"  # <- –ø–æ–¥—Å—Ç–∞–≤—å –ø—É—Ç—å –∫ —Å–≤–æ–µ–π train/audio\n",
    "output_csv  = \"E:/with_transcripts/TAUKADIAL-24/TAUKADIAL-24/transcripts_asr_openai_whisper-large-v3-turbo/TAUKADIAL_24_labels.csv\"      # <- –∫—É–¥–∞ —Å–æ—Ö—Ä–∞–Ω—è—Ç—å CSV (–º–æ–∂–Ω–æ —Ä—è–¥–æ–º)\n",
    "\n",
    "dataset_name = \"TAUKADIAL_24\"  # <- —Ç—ã —ç—Ç–æ —Å–∞–º –º–µ–Ω—è–µ—à—å, –ø–æ–Ω—è–ª?\n",
    "\n",
    "results = []\n",
    "\n",
    "for root, dirs, files in os.walk(input_root):\n",
    "    for file in files:\n",
    "        if file.endswith(\".json\"):\n",
    "            input_path = os.path.join(root, file)\n",
    "            json_filename = os.path.splitext(file)[0]\n",
    "            \n",
    "            # –≤–æ—Ç —Ç—É—Ç –≤—Å—ë –º–µ–Ω—è–µ—Ç—Å—è\n",
    "            class_name = os.path.relpath(root, input_root).replace(\"\\\\\", \"/\")\n",
    "\n",
    "            try:\n",
    "                with open(input_path, \"r\", encoding=\"utf-8\") as f:\n",
    "                    data = json.load(f)\n",
    "            except json.JSONDecodeError:\n",
    "                print(f\"ü§¢ –ì–Ω–∏–ª–æ–π —Ñ–∞–π–ª: {input_path}\")\n",
    "                continue\n",
    "\n",
    "            is_dialogue = False\n",
    "            dialog_parts = []\n",
    "\n",
    "            for entry in data:\n",
    "                speaker = entry.get(\"speaker\", \"unknown\")\n",
    "                words = [w[\"word\"] for w in entry.get(\"words\", [])]\n",
    "                if not words:\n",
    "                    continue\n",
    "                sentence = \" \".join(words).strip()\n",
    "\n",
    "                if speaker == \"unknown\":\n",
    "                    dialog_parts.append(f\"{sentence}.\")\n",
    "                else:\n",
    "                    is_dialogue = True\n",
    "                    dialog_parts.append(f\"{speaker}: {sentence}\")\n",
    "\n",
    "            full_text = \" \".join(dialog_parts) + \".\" if is_dialogue else \" \".join(dialog_parts)\n",
    "\n",
    "            results.append({\n",
    "                \"dataset\": dataset_name,\n",
    "                \"class\": class_name,  # <- —Ç–µ–ø–µ—Ä—å —ç—Ç–æ —á—Ç–æ-—Ç–æ —Ç–∏–ø–∞ test/cc\n",
    "                \"file\": json_filename,\n",
    "                \"sentence\": full_text\n",
    "            })\n",
    "            \n",
    "# –°–æ—Ö—Ä–∞–Ω—è–µ–º CSV\n",
    "os.makedirs(os.path.dirname(output_csv), exist_ok=True)\n",
    "\n",
    "with open(output_csv, \"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "    writer = csv.DictWriter(f, fieldnames=[\"dataset\", \"class\", \"file\", \"sentence\"])\n",
    "    writer.writeheader()\n",
    "    for row in results:\n",
    "        writer.writerow(row)\n",
    "\n",
    "print(f\"üçæ CSV –≥–æ—Ç–æ–≤: {output_csv} ‚Äî –∏ —Ç–µ–ø–µ—Ä—å —Å –∫–æ–ª–æ–Ω–∫–æ–π 'dataset', —á—Ç–æ–±—ã –∂–∏–∑–Ω—å –±—ã–ª–∞ –ª–æ–≥–∏—á–Ω–µ–µ, —á–µ–º —ç—Ç–æ—Ç –ø—Ä–æ–µ–∫—Ç.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d0fa2116-ebb5-4d9d-8dc1-a26b4869c21e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üóÇÔ∏è –í—Å—ë –∞–∫–∫—É—Ä–∞—Ç–Ω–æ —Ä–∞—Å–∫–∏–¥–∞–Ω–æ. –î–∞–∂–µ —Ç–≤–æ–π –¥–∏–∫–∏–π txt –≤—ã–∂–∏–ª.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import shutil\n",
    "import csv\n",
    "\n",
    "# –í–•–û–î–ù–´–ï –ü–ê–†–ê–ú–ï–¢–†–´\n",
    "source_folder = \"E:/with_transcripts/ADReSS-IS2020-data/ADReSS-IS2020-data/transcripts_asr_openai_whisper-large-v3-turbo/test_train/test\"     # –≥–¥–µ –≤—Å—ë –≤–ø–µ—Ä–µ–º–µ—à–∫—É\n",
    "label_file = \"E:/with_transcripts/ADReSS-IS2020-data/ADReSS-IS2020-data/test/adress_meta_data_test.txt\"  # –ø—É—Ç—å –∫ —Ñ–∞–π–ª—É —Å ID –∏ Label\n",
    "output_root = \"E:/with_transcripts/ADReSS-IS2020-data/ADReSS-IS2020-data/transcripts_asr_openai_whisper-large-v3-turbo/test_train/test_sorted\"     # –∫—É–¥–∞ —Å–æ—Ö—Ä–∞–Ω–∏—Ç—å cc/ –∏ cd/\n",
    "\n",
    "label_map = {}\n",
    "\n",
    "# –ü–∞—Ä—Å–∏–º —ç—Ç–æ—Ç 'txt-csv-–≥–∏–¥—Ä—É'\n",
    "with open(label_file, \"r\", encoding=\"utf-8\") as f:\n",
    "    lines = f.readlines()\n",
    "\n",
    "# –ü—Ä–æ–ø—É—Å–∫–∞–µ–º –∑–∞–≥–æ–ª–æ–≤–æ–∫\n",
    "for line in lines[1:]:\n",
    "    parts = [x.strip() for x in line.strip().split(\";\")]\n",
    "    if len(parts) < 4:\n",
    "        continue\n",
    "    sample_id = parts[0]\n",
    "    label_raw = parts[3]\n",
    "    label = \"cc\" if label_raw == \"1\" else \"cd\"\n",
    "    label_map[sample_id] = label\n",
    "\n",
    "# –°–æ–∑–¥–∞—ë–º –ø–∞–ø–∫–∏\n",
    "for label in set(label_map.values()):\n",
    "    os.makedirs(os.path.join(output_root, label), exist_ok=True)\n",
    "\n",
    "# –ò—â–µ–º –∏ –¥–≤–∏–≥–∞–µ–º —Ñ–∞–π–ª—ã –ø–æ ID\n",
    "for sample_id, label in label_map.items():\n",
    "    matched_files = [f for f in os.listdir(source_folder) if f.startswith(sample_id)]\n",
    "    if not matched_files:\n",
    "        print(f\"‚ùå –ù–µ –Ω–∞–π–¥–µ–Ω–æ —Ñ–∞–π–ª–æ–≤ –¥–ª—è ID: {sample_id}\")\n",
    "        continue\n",
    "    for filename in matched_files:\n",
    "        src = os.path.join(source_folder, filename)\n",
    "        dst = os.path.join(output_root, label, filename)\n",
    "        shutil.copy2(src, dst)\n",
    "\n",
    "print(\"üóÇÔ∏è –í—Å—ë –∞–∫–∫—É—Ä–∞—Ç–Ω–æ —Ä–∞—Å–∫–∏–¥–∞–Ω–æ. –î–∞–∂–µ —Ç–≤–æ–π –¥–∏–∫–∏–π txt –≤—ã–∂–∏–ª.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c3baf2d8-2997-4d0d-a72b-a5e3a8de461a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# –ü–∞–ø–∫–∞, –≥–¥–µ –ª–µ–∂–∞—Ç –≤—Å–µ —Ç–≤–æ–∏ CSV\n",
    "csv_folder = \"E:/with_transcripts/all_labels\" \n",
    "output_parquet = \"E:/with_transcripts/all_labels/merged_labels.parquet\"\n",
    "\n",
    "# –°–ø–∏—Å–æ–∫ –≤—Å–µ—Ö CSV-—Ñ–∞–π–ª–æ–≤\n",
    "csv_files = [f for f in os.listdir(csv_folder) if f.endswith(\".csv\")]\n",
    "\n",
    "# –ó–∞–≥—Ä—É–∑–∫–∞ –±–µ–∑ source_file\n",
    "df_list = []\n",
    "for csv_file in csv_files:\n",
    "    path = os.path.join(csv_folder, csv_file)\n",
    "    try:\n",
    "        df = pd.read_csv(path)\n",
    "        df_list.append(df)\n",
    "    except Exception as e:\n",
    "        print(f\"üß® –ù–µ –º–æ–≥—É –ø—Ä–æ—á–∏—Ç–∞—Ç—å {csv_file}: {e}\")\n",
    "\n",
    "# –û–±—ä–µ–¥–∏–Ω–µ–Ω–∏–µ\n",
    "merged_df = pd.concat(df_list, ignore_index=True)\n",
    "\n",
    "# –ö–∞—Å—Ç—É–µ–º file –≤ —Å—Ç—Ä–æ–∫—É\n",
    "if \"file\" in merged_df.columns:\n",
    "    merged_df[\"file\"] = merged_df[\"file\"].astype(str)\n",
    "\n",
    "# –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –≤ Parquet\n",
    "merged_df.to_parquet(output_parquet, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bdd48f28-c8bb-46a2-83b3-30f7e77893db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üéØ –ì–æ—Ç–æ–≤–æ. –¢–µ–ø–µ—Ä—å —Ñ–∞–π–ª—ã —Ç–∞–º, –≥–¥–µ –∏–º –∏ –º–µ—Å—Ç–æ.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import shutil\n",
    "import pandas as pd\n",
    "\n",
    "# –ü—É—Ç—å –∫ –ø–∞–ø–∫–µ, –≥–¥–µ –ª–µ–∂–∞—Ç –≤—Å–µ —Ñ–∞–π–ª—ã\n",
    "# source_folder = \"E:/with_transcripts/TAUKADIAL-24/TAUKADIAL-24/transcripts_asr_openai_whisper-large-v3-turbo/train/\"\n",
    "source_folder = \"E:/with_transcripts/TAUKADIAL-24/TAUKADIAL-24/transcripts_asr_openai_whisper-large-v3-turbo/test/\"\n",
    "\n",
    "\n",
    "# –ü—É—Ç—å, –∫—É–¥–∞ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª—è—Ç—å\n",
    "# output_root = \"E:/with_transcripts/TAUKADIAL-24/TAUKADIAL-24/transcripts_asr_openai_whisper-large-v3-turbo/train/\"\n",
    "output_root = \"E:/with_transcripts/TAUKADIAL-24/TAUKADIAL-24/transcripts_asr_openai_whisper-large-v3-turbo/tst/\"\n",
    "\n",
    "\n",
    "# CSV —Å –∏–º–µ–Ω–∞–º–∏ –∏ –∫–ª–∞—Å—Å–∞–º–∏\n",
    "csv_path = \"E:/with_transcripts/TAUKADIAL-24/TAUKADIAL_testgroundtruth.csv\"\n",
    "\n",
    "# –ß–∏—Ç–∞–µ–º CSV\n",
    "df = pd.read_csv(csv_path,  sep=\";\")\n",
    "\n",
    "# –°–æ–∑–¥–∞—ë–º –æ—Ç–æ–±—Ä–∞–∂–µ–Ω–∏–µ: {basename: label}\n",
    "file_label_map = dict(zip(df[\"tkdname\"].str.replace(r\"\\..*$\", \"\", regex=True), df[\"dx\"]))\n",
    "\n",
    "# –ò–Ω–¥–µ–∫—Å–∏—Ä—É–µ–º –≤—Å—ë, —á—Ç–æ –ª–µ–∂–∏—Ç –≤ –ø–∞–ø–∫–µ (—Ñ–∞–π–ª—ã —Å —Ä–∞—Å—à–∏—Ä–µ–Ω–∏—è–º–∏)\n",
    "all_files = os.listdir(source_folder)\n",
    "\n",
    "# –†–∞—Å–∫–∏–¥—ã–≤–∞–µ–º\n",
    "for base_name, label in file_label_map.items():\n",
    "    # –∏—â–µ–º –≤—Å–µ —Ñ–∞–π–ª—ã, –∫–æ—Ç–æ—Ä—ã–µ –Ω–∞—á–∏–Ω–∞—é—Ç—Å—è —Å base_name\n",
    "    matches = [f for f in all_files if os.path.splitext(f)[0] == base_name]\n",
    "\n",
    "    if not matches:\n",
    "        print(f\"‚ùå –§–∞–π–ª –Ω–µ –Ω–∞–π–¥–µ–Ω –¥–ª—è ID: {base_name}\")\n",
    "        continue\n",
    "\n",
    "    label_folder = os.path.join(output_root, label)\n",
    "    os.makedirs(label_folder, exist_ok=True)\n",
    "\n",
    "    for match in matches:\n",
    "        src = os.path.join(source_folder, match)\n",
    "        dst = os.path.join(label_folder, match)\n",
    "        shutil.copy2(src, dst)\n",
    "\n",
    "print(\"üéØ –ì–æ—Ç–æ–≤–æ. –¢–µ–ø–µ—Ä—å —Ñ–∞–π–ª—ã —Ç–∞–º, –≥–¥–µ –∏–º –∏ –º–µ—Å—Ç–æ.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "28c91b9c-f325-4204-a56a-cc4045e3572b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìå –ì–æ—Ç–æ–≤–æ. –ö–æ–ª–æ–Ω–∫–∞ 'dx' –æ—Å—Ç–∞–ª–∞—Å—å –Ω–∞ –º–µ—Å—Ç–µ, –±—É–¥—Ç–æ –Ω–∏—á–µ–≥–æ –∏ –Ω–µ –±—ã–ª–æ: E:/with_transcripts/TAUKADIAL_24_labels_corrected.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# –ü—É—Ç–∏ –∫ —Ñ–∞–π–ª–∞–º\n",
    "cleaned_path = \"E:/with_transcripts/all_labels/TAUKADIAL_24_labels_cleaned.csv\"\n",
    "dirty_path = \"E:/with_transcripts/TAUKADIAL-24/TAUKADIAL-24/transcripts_asr_openai_whisper-large-v3-turbo/TAUKADIAL_24_labels.csv\"\n",
    "output_path = \"E:/with_transcripts/TAUKADIAL_24_labels_corrected.csv\"\n",
    "\n",
    "# –ó–∞–≥—Ä—É–∑–∫–∞\n",
    "df_clean = pd.read_csv(cleaned_path)\n",
    "df_dirty = pd.read_csv(dirty_path)\n",
    "\n",
    "# –°–æ–∑–¥–∞—ë–º –º–∞–ø–ø–∏–Ω–≥: tkdname -> dx (–∏–∑ –≥—Ä—è–∑–Ω–æ–≥–æ)\n",
    "label_map = dict(zip(df_dirty[\"file\"], df_dirty[\"class\"]))\n",
    "\n",
    "# –û–±–Ω–æ–≤–∏–º –∑–Ω–∞—á–µ–Ω–∏—è –≤ df_clean['dx'] –ø–æ –º–∞–ø–ø–∏–Ω–≥—É\n",
    "if \"class\" in df_clean.columns:\n",
    "    df_clean[\"class\"] = df_clean[\"file\"].map(label_map)\n",
    "else:\n",
    "    raise ValueError(\"–ö–æ–ª–æ–Ω–∫–∞ 'class' –Ω–µ –Ω–∞–π–¥–µ–Ω–∞ –≤ –æ—á–∏—â–µ–Ω–Ω–æ–º —Ñ–∞–π–ª–µ. –Ø –≤ —Ç—Ä–∞—É—Ä–µ.\")\n",
    "\n",
    "# –°–æ—Ö—Ä–∞–Ω—è–µ–º —Å —Ç–µ–º –∂–µ –ø–æ—Ä—è–¥–∫–æ–º –∫–æ–ª–æ–Ω–æ–∫\n",
    "df_clean.to_csv(output_path, index=False)\n",
    "\n",
    "print(f\"üìå –ì–æ—Ç–æ–≤–æ. –ö–æ–ª–æ–Ω–∫–∞ 'dx' –æ—Å—Ç–∞–ª–∞—Å—å –Ω–∞ –º–µ—Å—Ç–µ, –±—É–¥—Ç–æ –Ω–∏—á–µ–≥–æ –∏ –Ω–µ –±—ã–ª–æ: {output_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69eb95bc-0e58-4436-8ff9-77d9284936dc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
