import logging
import torch
import random
import os
import shutil
import datetime

from torch.utils.data import DataLoader
import torch.nn as nn
import torch.optim as optim
import torch.nn.functional as F
from tqdm import tqdm
import numpy as np

from utils.logger_setup import setup_logger
from utils.config_loader import ConfigLoader
from utils.losses import WeightedCrossEntropyLoss
from utils.measures import uar, war, mf1, wf1
from models.models import BiFormer, BiGraphFormer, MultiModalTransformer_v5, MultiModalTransformer_v4, MultiModalTransformer_v3
from data_loading.dataset_multimodal import DatasetMultiModal
from data_loading.feature_extractor import AudioEmbeddingExtractor, TextEmbeddingExtractor


def custom_collate_fn(batch):
    """–°–æ–±–∏—Ä–∞–µ—Ç —Å–ø–∏—Å–æ–∫ –æ–±—Ä–∞–∑—Ü–æ–≤ –≤ –µ–¥–∏–Ω—ã–π –±–∞—Ç—á, –æ—Ç–±—Ä–∞—Å—ã–≤–∞—è None (–Ω–µ–≤–∞–ª–∏–¥–Ω—ã–µ)."""
    batch = [x for x in batch if x is not None]
    if not batch:
        return None

    audios = [b["audio"] for b in batch]   # list of (1, samples) tensors
    audio_tensor = torch.stack(audios)     # (B, 1, samples)

    labels = [b["label"] for b in batch]   # list of (num_emotions,) (one-hot)
    label_tensor = torch.stack(labels)     # (B, num_emotions)

    texts = [b["text"] for b in batch]     # list of str

    return {
        "audio": audio_tensor,  # (B, 1, samples)
        "label": label_tensor,  # (B, num_emotions)
        "text": texts           # list[str] –¥–ª–∏–Ω–æ–π B
    }


def make_dataset_and_loader(config, split: str):
    """
    –°–æ–∑–¥–∞—ë—Ç (dataset, dataloader) –¥–ª—è —É–∫–∞–∑–∞–Ω–Ω–æ–≥–æ —Å–ø–ª–∏—Ç–∞: train/dev/test.

    –ß–∏—Ç–∞–µ—Ç config.csv_path / config.wav_dir –∫–∞–∫ —à–∞–±–ª–æ–Ω—ã —Å {split}.
    –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç (dataset, dataloader).
    """
    csv_path = config.csv_path.format(base_dir=config.base_dir, split=split)
    wav_dir  = config.wav_dir.format(base_dir=config.base_dir,  split=split)

    print(csv_path, wav_dir, split)

    dataset = DatasetMultiModal(
        csv_path = csv_path,
        wav_dir  = wav_dir,
        emotion_columns=config.emotion_columns,
        split          =split,
        sample_rate    =config.sample_rate,
        wav_length     =config.wav_length,
        whisper_model  =config.whisper_model,
        text_column    =config.text_column,
        use_whisper_for_nontrain_if_no_text=config.use_whisper_for_nontrain_if_no_text,
        whisper_device =config.whisper_device,
        subset_size    =config.subset_size,
        merge_probability=config.merge_probability
    )

    # –î–ª—è train –æ–±—ã—á–Ω–æ shuffle=True, dev/test ‚Äî –æ–±—ã—á–Ω–æ False
    shuffle = (split == "train")

    dataloader = DataLoader(
        dataset,
        batch_size  = config.batch_size,
        shuffle     = shuffle,
        num_workers = config.num_workers,
        collate_fn  = custom_collate_fn
    )
    return dataset, dataloader


def run_eval(model, loader, audio_extractor, text_extractor, criterion, device="cuda"):
    """
    –û—Ü–µ–Ω–∏–≤–∞–µ–º (–≤–∞–ª–∏–¥–∞—Ü–∏—è/—Ç–µ—Å—Ç) –Ω–∞ –∑–∞–¥–∞–Ω–Ω–æ–º loader'–µ, –≤–æ–∑–≤—Ä–∞—â–∞–µ–º (loss, accuracy).
    """
    model.eval()
    total_loss = 0.0
    correct    = 0
    total      = 0
    total_preds = []
    total_targets = []

    with torch.no_grad():
        for batch in tqdm(loader):
            if batch is None:
                continue

            audio  = batch["audio"].to(device)
            labels = batch["label"].to(device)
            texts  = batch["text"]

            # –ò–∑–≤–ª–µ–∫–∞–µ–º —ç–º–±–µ–¥–¥–∏–Ω–≥–∏
            audio_emb = audio_extractor.extract(audio)
            text_emb  = text_extractor.extract(texts)

            # –ü—Ä–æ–≥–æ–Ω—è–µ–º —á–µ—Ä–µ–∑ –º–æ–¥–µ–ª—å
            logits = model(audio_emb, text_emb)

            # –°—á–∏—Ç–∞–µ–º –ª–æ—Å—Å
            target = labels.argmax(dim=1)  # (B,)
            loss = criterion(logits, target)

            # –ù–∞–∫–æ–ø–∏–º
            bs = audio.shape[0]
            total_loss += loss.item() * bs

            # accuracy
            preds = logits.argmax(dim=1)
            # correct += (preds == target).sum().item()
            total_preds.extend(preds.cpu().numpy().tolist())
            total_targets.extend(target.cpu().numpy().tolist())
            total   += bs

    avg_loss = total_loss / total

    uar_m = uar(total_targets, total_preds)
    war_m = war(total_targets, total_preds)
    mf1_m = mf1(total_targets, total_preds)
    wf1_m = wf1(total_targets, total_preds)
    # accuracy = correct / total
    return avg_loss, uar_m, war_m, mf1_m, wf1_m


def main():
    # –õ–æ–≥-—Ñ–∞–π–ª
    os.makedirs("logs", exist_ok=True)
    datestr  = datetime.datetime.now().strftime("%Y-%m-%d_%H-%M-%S")
    log_file = os.path.join("logs", f"train_log_{datestr}.txt")
    setup_logger(logging.INFO, log_file=log_file)

    logging.info("üöÄ === –ó–∞–ø—É—Å–∫ —Ç—Ä–µ–Ω–∏—Ä–æ–≤–∫–∏ (train/dev/test) ===")

    # –ó–∞–≥—Ä—É–∂–∞–µ–º –∫–æ–Ω—Ñ–∏–≥
    config = ConfigLoader("config.toml")
    config.show_config()

    # –§–∏–∫—Å–∏—Ä—É–µ–º seed
    if config.random_seed > 0:
        random.seed(config.random_seed)
        torch.manual_seed(config.random_seed)
        logging.info(f"üîí –§–∏–∫—Å–∏—Ä—É–µ–º random seed: {config.random_seed}")
    else:
        logging.info("üîì Random seed –Ω–µ —Ñ–∏–∫—Å–∏—Ä–æ–≤–∞–Ω (0).")

    device = "cuda" if torch.cuda.is_available() else "cpu"
    # –ï—Å–ª–∏ –≤ config –µ—Å—Ç—å –æ—Ç–¥–µ–ª—å–Ω–æ–µ –ø–æ–ª–µ emb_device, —Ç–æ –º–æ–∂–Ω–æ device = config.emb_device

    # –°–æ–∑–¥–∞—ë–º –¥–∞—Ç–∞—Å–µ—Ç—ã/–ª–æ–∞–¥–µ—Ä—ã –¥–ª—è train/dev/test
    _, train_loader = make_dataset_and_loader(config, "train")
    _, dev_loader   = make_dataset_and_loader(config, "dev")
    _, test_loader  = make_dataset_and_loader(config, "test")

    # –≠–∫—Å—Ç—Ä–∞–∫—Ç–æ—Ä—ã —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤
    audio_extractor = AudioEmbeddingExtractor(config)
    text_extractor  = TextEmbeddingExtractor(config)

    # –ü—Ä–µ–¥–ø–æ–ª–æ–∂–∏–º, —á—Ç–æ —ç–∫—Å—Ç—Ä–∞–∫—Ç–æ—Ä—ã –≤—ã–¥–∞—é—Ç —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å = 1024
    hidden_dim=config.hidden_dim
    num_classes=len(config.emotion_columns)
    num_transformer_heads=config.num_transformer_heads
    num_graph_heads=config.num_graph_heads
    mode=config.mode
    positional_encoding=config.positional_encoding
    dropout=config.dropout
    out_features=config.out_features
    lr=config.lr
    num_epochs=config.num_epochs
    tr_layer_number=config.tr_layer_number

    dict_models = {
                        'BiFormer': BiFormer,
                        'BiGraphFormer': BiGraphFormer,
                        'MultiModalTransformer_v5': MultiModalTransformer_v5,
                        'MultiModalTransformer_v4': MultiModalTransformer_v4,
                        'MultiModalTransformer_v3': MultiModalTransformer_v3
                    }

    model_cls = dict_models[config.model_name]


    model = model_cls(audio_dim=config.audio_embedding_dim,
                                text_dim=config.text_embedding_dim,
                                hidden_dim=hidden_dim,
                                num_transformer_heads=num_transformer_heads,
                                num_graph_heads=num_graph_heads,
                                seg_len=config.max_tokens,
                                mode=mode,
                                dropout=dropout,
                                positional_encoding=positional_encoding,
                                out_features=out_features,
                                tr_layer_number=tr_layer_number,
                                device=device,
                                num_classes=num_classes).to(device)

    # –û–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä, –ª–æ—Å—Å
    optimizer = optim.Adam(model.parameters(), lr=lr)
    criterion = WeightedCrossEntropyLoss()

    for epoch in range(num_epochs):
        logging.info(f"\n=== –≠–ø–æ—Ö–∞ {epoch} ===")

        # --- TRAIN ---
        model.train()
        total_loss = 0.0
        total_samples = 0
        total_preds = []
        total_targets = []
        for batch in tqdm(train_loader):
            if batch is None:
                continue

            audio  = batch["audio"].to(device)
            labels = batch["label"].to(device)
            texts  = batch["text"]

            # –ò–∑–≤–ª–µ–∫–∞–µ–º —ç–º–±–µ–¥–¥–∏–Ω–≥–∏
            audio_emb = audio_extractor.extract(audio)
            text_emb  = text_extractor.extract(texts)

            # –ü—Ä–æ–≥–æ–Ω—è–µ–º —á–µ—Ä–µ–∑ –º–æ–¥–µ–ª—å
            logits = model(audio_emb, text_emb)

            # –õ–æ—Å—Å
            target = labels.argmax(dim=1)  # (B,)
            loss   = criterion(logits, target)

            optimizer.zero_grad()
            loss.backward()
            optimizer.step()

            bs = audio.shape[0]
            total_loss += loss.item() * bs

            # accuracy
            preds = logits.argmax(dim=1)
            total_preds.extend(preds.cpu().numpy().tolist())
            total_targets.extend(target.cpu().numpy().tolist())
            total_samples += bs

        train_loss = total_loss / total_samples
        uar_m = uar(total_targets, total_preds)
        war_m = war(total_targets, total_preds)
        mf1_m = mf1(total_targets, total_preds)
        wf1_m = wf1(total_targets, total_preds)
        meam_m = np.mean([uar_m,war_m, mf1_m, wf1_m])

        logging.info(f"[TRAIN] Loss={train_loss:.4f}, UAR={uar_m:.4f}, WAR={war_m:.4f}, MF1={mf1_m:.4f}, WF1={wf1_m:.4f}, MEAN={meam_m:.4f}")

        # --- DEV ---
        dev_loss, dev_uar_m, dev_war_m, dev_mf1_m, dev_wf1_m  = run_eval(model, dev_loader, audio_extractor, text_extractor, criterion, device)
        dev_meam_m = np.mean([dev_uar_m,dev_war_m, dev_mf1_m, dev_wf1_m])
        logging.info(f"[DEV]   Loss={dev_loss:.4f}, UAR={dev_uar_m:.4f}, WAR={dev_war_m:.4f}, MF1={dev_mf1_m:.4f}, WF1={dev_wf1_m:.4f}, MEAN={dev_meam_m:.4f}")

        # –ü–æ—Å–ª–µ –æ–∫–æ–Ω—á–∞–Ω–∏—è —ç–ø–æ—Ö ‚Äî –ø—Ä–æ–≥–æ–Ω—è–µ–º test (–æ–¥–∏–Ω —Ä–∞–∑)
        test_loss, test_uar_m, test_war_m, test_mf1_m, test_wf1_m = run_eval(model, test_loader, audio_extractor, text_extractor, criterion, device)
        test_meam_m = np.mean([test_uar_m,test_war_m, test_mf1_m, test_wf1_m])
        logging.info(f"[TEST]  Loss={test_loss:.4f}, UAR={test_uar_m:.4f}, WAR={test_war_m:.4f}, MF1={test_mf1_m:.4f}, WF1={test_wf1_m:.4f}, MEAN={test_meam_m:.4f}")

    logging.info("‚úÖ –¢—Ä–µ–Ω–∏—Ä–æ–≤–∫–∞ –∑–∞–≤–µ—Ä—à–µ–Ω–∞. –í—Å–µ split'—ã –æ–±—Ä–∞–±–æ—Ç–∞–Ω—ã!")

    results_dir = f"results_{datestr}"
    os.makedirs(results_dir, exist_ok=True)

    shutil.copy(
        "config.toml",
        os.path.join(results_dir, f"config_{datestr}.toml")
    )

if __name__ == "__main__":
    main()
