{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "70ad8b1d-9a7e-4470-8767-ee0d3bc01c11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Подавление предупреждений\n",
    "import warnings\n",
    "for warn in [UserWarning, FutureWarning]: warnings.filterwarnings(\"ignore\", category = warn)\n",
    "\n",
    "# Импорт необходимых библиотек\n",
    "import os\n",
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModel,AutoModelForMaskedLM, RobertaModel, RobertaTokenizer\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch import Tensor\n",
    "from einops import rearrange\n",
    "from typing import Tuple, Callable\n",
    "from torch.autograd import Function\n",
    "import gc\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "65ea47c2-991e-4186-9650-58eb3c9cd691",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Embedding():\n",
    "    def __init__(self, model_name='jina', pooling=None):\n",
    "        self.model_name = model_name\n",
    "        self.pooling = pooling\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        if model_name == 'jina':\n",
    "            self.tokenizer = AutoTokenizer.from_pretrained(\"jinaai/jina-embeddings-v3\", code_revision='da863dd04a4e5dce6814c6625adfba87b83838aa', trust_remote_code=True)\n",
    "            self.model = AutoModel.from_pretrained(\"jinaai/jina-embeddings-v3\", code_revision='da863dd04a4e5dce6814c6625adfba87b83838aa', trust_remote_code=True).to(self.device)\n",
    "        elif model_name == 'xlm-roberta-base':\n",
    "            self.tokenizer = AutoTokenizer.from_pretrained('xlm-roberta-base')\n",
    "            self.model = AutoModel.from_pretrained('xlm-roberta-base').to(self.device)\n",
    "        elif model_name == 'canine-c':\n",
    "            self.tokenizer = AutoTokenizer.from_pretrained('google/canine-c')\n",
    "            self.model = AutoModel.from_pretrained('google/canine-c').to(self.device)\n",
    "        else:\n",
    "            raise ValueError('Unknown name of Embedding')\n",
    "    def _mean_pooling(self, X):\n",
    "        def mean_pooling(model_output, attention_mask):\n",
    "            token_embeddings = model_output[0]\n",
    "            input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n",
    "            return torch.sum(token_embeddings * input_mask_expanded, 1) / torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n",
    "        encoded_input = self.tokenizer(X, padding=True, truncation=True, return_tensors='pt').to(self.device)\n",
    "        with torch.no_grad():\n",
    "            model_output = self.model(**encoded_input)\n",
    "        sentence_embeddings = mean_pooling(model_output, encoded_input['attention_mask'])\n",
    "        sentence_embeddings = F.normalize(sentence_embeddings, p=2, dim=1)\n",
    "        return sentence_embeddings.unsqueeze(1)\n",
    "    \n",
    "    def get_embeddings(self, X):\n",
    "        if self.pooling is None:\n",
    "            if self.model_name == 'canine-c_emb':\n",
    "                max_len = 329\n",
    "            else:\n",
    "                max_len = 95\n",
    "            encoded_input = self.tokenizer(X, padding=True, truncation=True, return_tensors='pt').to(self.device)\n",
    "            with torch.no_grad():\n",
    "                features = self.model(**encoded_input)[0].detach().cpu().float().numpy()\n",
    "            res = np.pad(features[:, :max_len, :], ((0, 0), (0, max(0, max_len - features.shape[1])), (0, 0)), \"constant\")\n",
    "            return torch.tensor(res)\n",
    "        elif self.pooling == 'mean':\n",
    "            return self._mean_pooling(X)\n",
    "        else:\n",
    "            raise ValueError('Unknown type of pooling')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "935f444d-ee62-46d2-af78-b3991120b699",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PScan(Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, A_inp, X_inp):\n",
    "        A, X = A_inp.clone(), X_inp.clone()\n",
    "        A, X = rearrange(A, \"l b d s -> b d l s\"), rearrange(X, \"l b d s -> b d l s\")\n",
    "        PScan._forward(A, X)\n",
    "        ctx.save_for_backward(A.clone(), X)\n",
    "        return rearrange(X, \"b d l s -> b l d s\")\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_inp: Tensor) -> Tuple[Tensor, Tensor]:\n",
    "        A, X = ctx.saved_tensors\n",
    "        A = torch.cat((A[:, :, :1], A[:, :, 1:].flip(2)), dim = 2)\n",
    "        grad_out = rearrange(grad_inp, \"b l d s -> b d l s\")\n",
    "        grad_out = grad_out.flip(2)\n",
    "        PScan._forward(A, grad_out)\n",
    "        grad_out = grad_out.flip(2)\n",
    "        Q = torch.zeros_like(X)\n",
    "        Q[:, :, 1:].add_(X[:, :, :-1] * grad_out[:, :, 1:])\n",
    "        return rearrange(Q, \"b d l s -> b l d s\"), rearrange(grad_out, \"b d l s -> b l d s\")\n",
    "\n",
    "    @staticmethod\n",
    "    def _forward(A: Tensor, X: Tensor) -> None:\n",
    "        b, d, l, s = A.shape\n",
    "        num_steps = int(math.log2(l))\n",
    "        Av, Xv = A, X\n",
    "        for _ in range(num_steps):\n",
    "            T = Xv.size(2)\n",
    "            Av, Xv = Av[:, :, :T].reshape(b, d, T // 2, 2, -1), Xv[:, :, :T].reshape(b, d, T // 2, 2, -1)\n",
    "            Xv[:, :, :, 1].add_(Av[:, :, :, 1].mul(Xv[:, :, :, 0]))\n",
    "            Av[:, :, :, 1].mul_(Av[:, :, :, 0])\n",
    "            Av, Xv = Av[:, :, :, 1], Xv[:, :, :, 1]\n",
    "        for k in range(num_steps - 1, -1, -1):\n",
    "            Av, Xv = A[:, :, 2**k - 1 : l : 2**k], X[:, :, 2**k - 1 : l : 2**k]\n",
    "            T = 2 * (Xv.size(2) // 2)\n",
    "            if T < Xv.size(2):\n",
    "                Xv[:, :, -1].add_(Av[:, :, -1].mul(Xv[:, :, -2]))\n",
    "                Av[:, :, -1].mul_(Av[:, :, -2])\n",
    "            Av, Xv = Av[:, :, :T].reshape(b, d, T // 2, 2, -1), Xv[:, :, :T].reshape(b, d, T // 2, 2, -1)\n",
    "            Xv[:, :, 1:, 0].add_(Av[:, :, 1:, 0].mul(Xv[:, :, :-1, 1]))\n",
    "            Av[:, :, 1:, 0].mul_(Av[:, :, :-1, 1])\n",
    "\n",
    "pscan: Callable[[Tensor, Tensor], Tensor] = PScan.apply\n",
    "\n",
    "class RMSNorm(nn.Module):\n",
    "    def __init__(self, d_model: int, eps: float = 1e-8) -> None:\n",
    "        super().__init__()\n",
    "        self.eps = eps\n",
    "        self.weight = nn.Parameter(torch.ones(d_model))\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:        \n",
    "        return x * torch.rsqrt(x.pow(2).mean(-1, keepdim = True) + self.eps) * self.weight\n",
    "\n",
    "class MambaBlock(nn.Module):\n",
    "    def __init__(self, d_input, d_model):\n",
    "        super(MambaBlock, self).__init__()\n",
    "        self.in_proj = nn.Linear(d_input, d_model)\n",
    "        self.s_B = nn.Linear(d_model, d_model)\n",
    "        self.s_C = nn.Linear(d_model, d_model)\n",
    "        self.out_proj = nn.Linear(d_model, d_input)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.to(device)\n",
    "        x = self.in_proj(x)\n",
    "        B, C = self.s_B(x), self.s_C(x)\n",
    "        res = self.out_proj(x + B + C)\n",
    "        return res\n",
    "\n",
    "class Mamba(nn.Module):\n",
    "    def __init__(self, num_layers, d_input, d_model, num_classes, model_name='jina', pooling=None, transform_idx_to_labels=['anger', 'disgust', 'fear', 'sadness', 'neutral', 'happiness', 'enthusiasm']):\n",
    "        super(Mamba, self).__init__()\n",
    "        embed = Embedding(model_name, pooling)\n",
    "        self.embedding = embed.get_embeddings\n",
    "        self.layers = nn.ModuleList([MambaBlock(d_input, d_model) for _ in range(num_layers)])\n",
    "        self.fc_out = nn.Linear(d_input, num_classes)\n",
    "        self.model_name = model_name\n",
    "        self.transform_idx_to_labels = transform_idx_to_labels\n",
    "\n",
    "    def forward(self, seq):\n",
    "        seq = torch.tensor(self.embedding(seq)).to(device)\n",
    "        for mamba in self.layers:\n",
    "            seq = mamba(seq)\n",
    "        return self.fc_out(seq.mean(dim = 1))\n",
    "    def predict(self, x):\n",
    "        with torch.no_grad():\n",
    "            output = self.forward(x)\n",
    "            _, predictions = torch.max(output, dim=1)\n",
    "            result = [self.transform_idx_to_labels[i] for i in (map(int, predictions))]\n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "419fea30-ace3-4f5f-b234-d3a725c7b1c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "5928d398-f9d1-4eb2-bd5b-236286007faa",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture --no-stdout\n",
    "model = Mamba(model_name='jina', pooling=None, num_layers = 2, d_input = 1024, d_model = 256, num_classes=7, transform_idx_to_labels=['anger', 'disgust', 'fear', 'sadness', 'neutral', 'joy', 'surprise']).to(device)\n",
    "PATH_TO_MODEL = os.path.join(os.path.join(\".\"), \"models\")\n",
    "checkpoint = torch.load(os.path.join(PATH_TO_MODEL, \"MELD_checkpoint.pth\"))\n",
    "model.load_state_dict(checkpoint['model_state_dict'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "19d0b7fb-c503-4c7a-9fc4-f0598bf00267",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['neutral', 'surprise', 'neutral', 'anger', 'sadness']"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict(['hello', 'Wow! This for me??', 'I work as a teacher.', 'Go away, I hate you.', \"I have nothing, I'm a loser.\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "f4bcabd9-dbeb-46c8-b949-c30e2a4317e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture --no-stdout\n",
    "model = Mamba(model_name='jina', pooling=None, num_layers = 2, d_input = 1024, d_model = 64, num_classes=7).to(device)\n",
    "PATH_TO_MODEL = os.path.join(os.path.join(\".\"), \"models\")\n",
    "checkpoint = torch.load(os.path.join(PATH_TO_MODEL, \"RESD_checkpoint.pth\"))\n",
    "model.load_state_dict(checkpoint['model_state_dict'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "367d1c3d-61da-48eb-8fb4-c186cf637c2b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['anger', 'fear', 'anger', 'neutral', 'happiness']"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict(['Уходи, я тебя ненавижу', 'Мне страшно!', 'Как тут грязно, все в пыли и сырости', 'Я работаю учителем', 'Ура! Спасибо!'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Google Colab Analog 2024 (PyTorch 2.5.1 + TensorFlow 2.18) [python-google_colab_gpu_2024]",
   "language": "python",
   "name": "conda-env-python-google_colab_gpu_2024-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
