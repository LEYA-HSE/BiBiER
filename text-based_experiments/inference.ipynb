{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "70ad8b1d-9a7e-4470-8767-ee0d3bc01c11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Подавление предупреждений\n",
    "import warnings\n",
    "for warn in [UserWarning, FutureWarning]: warnings.filterwarnings(\"ignore\", category = warn)\n",
    "\n",
    "# Импорт необходимых библиотек\n",
    "import os\n",
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from torch.nn.functional import silu\n",
    "from torch.nn.functional import softplus\n",
    "from einops import rearrange, repeat, einsum\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModel,AutoModelForMaskedLM, RobertaModel, RobertaTokenizer\n",
    "from torch import Tensor\n",
    "from einops import rearrange\n",
    "from typing import Tuple, Callable\n",
    "from torch.autograd import Function\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "65ea47c2-991e-4186-9650-58eb3c9cd691",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Embedding():\n",
    "    def __init__(self, model_name='jina', pooling=None):\n",
    "        self.model_name = model_name\n",
    "        self.pooling = pooling\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        if model_name == 'jina':\n",
    "            self.tokenizer = AutoTokenizer.from_pretrained(\"jinaai/jina-embeddings-v3\", code_revision='da863dd04a4e5dce6814c6625adfba87b83838aa', trust_remote_code=True)\n",
    "            self.model = AutoModel.from_pretrained(\"jinaai/jina-embeddings-v3\", code_revision='da863dd04a4e5dce6814c6625adfba87b83838aa', trust_remote_code=True).to(self.device)\n",
    "        elif model_name == 'xlm-roberta-base':\n",
    "            self.tokenizer = AutoTokenizer.from_pretrained('xlm-roberta-base')\n",
    "            self.model = AutoModel.from_pretrained('xlm-roberta-base').to(self.device)\n",
    "        elif model_name == 'canine-c':\n",
    "            self.tokenizer = AutoTokenizer.from_pretrained('google/canine-c')\n",
    "            self.model = AutoModel.from_pretrained('google/canine-c').to(self.device)\n",
    "        else:\n",
    "            raise ValueError('Unknown name of Embedding')\n",
    "    def _mean_pooling(self, X):\n",
    "        def mean_pooling(model_output, attention_mask):\n",
    "            token_embeddings = model_output[0]\n",
    "            input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n",
    "            return torch.sum(token_embeddings * input_mask_expanded, 1) / torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n",
    "        encoded_input = self.tokenizer(X, padding=True, truncation=True, return_tensors='pt').to(self.device)\n",
    "        with torch.no_grad():\n",
    "            model_output = self.model(**encoded_input)\n",
    "        sentence_embeddings = mean_pooling(model_output, encoded_input['attention_mask'])\n",
    "        sentence_embeddings = F.normalize(sentence_embeddings, p=2, dim=1)\n",
    "        return sentence_embeddings.unsqueeze(1)\n",
    "    \n",
    "    def get_embeddings(self, X):\n",
    "        if self.pooling is None:\n",
    "            if self.model_name == 'canine-c_emb':\n",
    "                max_len = 329\n",
    "            else:\n",
    "                max_len = 95\n",
    "            encoded_input = self.tokenizer(X, padding=True, truncation=True, return_tensors='pt').to(self.device)\n",
    "            with torch.no_grad():\n",
    "                features = self.model(**encoded_input)[0].detach().cpu().float().numpy()\n",
    "            res = np.pad(features[:, :max_len, :], ((0, 0), (0, max(0, max_len - features.shape[1])), (0, 0)), \"constant\")\n",
    "            return torch.tensor(res)\n",
    "        elif self.pooling == 'mean':\n",
    "            return self._mean_pooling(X)\n",
    "        else:\n",
    "            raise ValueError('Unknown type of pooling')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4f23ad45-c4c0-4e11-bc9f-b90b746b8c30",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RMSNorm(nn.Module):\n",
    "    def __init__(self, d_model: int, eps: float = 1e-8) -> None:\n",
    "        super().__init__()\n",
    "        self.eps = eps\n",
    "        self.weight = nn.Parameter(torch.ones(d_model))\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:        \n",
    "        return x * torch.rsqrt(x.pow(2).mean(-1, keepdim = True) + self.eps) * self.weight\n",
    "\n",
    "class Mamba(nn.Module):\n",
    "    def __init__(self, num_layers, d_input, d_model, d_state=16, d_discr=None, ker_size=4, num_classes=7, model_name='jina', pooling=None):\n",
    "        super().__init__()\n",
    "        mamba_par = {\n",
    "            'd_input' : d_input,\n",
    "            'd_model' : d_model,\n",
    "            'd_state' : d_state,\n",
    "            'd_discr' : d_discr,\n",
    "            'ker_size': ker_size\n",
    "        }\n",
    "        self.model_name = model_name\n",
    "        embed = Embedding(model_name, pooling)\n",
    "        self.embedding = embed.get_embeddings\n",
    "        self.layers = nn.ModuleList([nn.ModuleList([MambaBlock(**mamba_par), RMSNorm(d_input)]) for _ in range(num_layers)])\n",
    "        self.fc_out = nn.Linear(d_input, num_classes)\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        \n",
    "    def forward(self, seq, cache=None):\n",
    "        seq = torch.tensor(self.embedding(seq)).to(self.device)\n",
    "        for mamba, norm in self.layers:\n",
    "            out, cache = mamba(norm(seq), cache)\n",
    "            seq = out + seq\n",
    "        return self.fc_out(seq.mean(dim = 1))\n",
    "    \n",
    "    def predict(self, x):\n",
    "        label_to_emotion = {\n",
    "            0: 'anger',\n",
    "            1: 'disgust',\n",
    "            2: 'fear',\n",
    "            3: 'joy/happiness',\n",
    "            4: 'neutral',\n",
    "            5: 'sadness',\n",
    "            6: 'surprise/enthusiasm'\n",
    "        }\n",
    "        with torch.no_grad():\n",
    "            output = self.forward(x)\n",
    "            _, predictions = torch.max(output, dim=1)\n",
    "            result = [label_to_emotion[i] for i in (map(int, predictions))]\n",
    "        return result\n",
    "        \n",
    "class MambaBlock(nn.Module):\n",
    "    def __init__(self, d_input, d_model, d_state=16, d_discr=None, ker_size=4):\n",
    "        super().__init__()\n",
    "        d_discr = d_discr if d_discr is not None else d_model // 16\n",
    "        self.in_proj  = nn.Linear(d_input, 2 * d_model, bias=False)\n",
    "        self.out_proj = nn.Linear(d_model, d_input, bias=False)\n",
    "        self.s_B = nn.Linear(d_model, d_state, bias=False)\n",
    "        self.s_C = nn.Linear(d_model, d_state, bias=False)\n",
    "        self.s_D = nn.Sequential(nn.Linear(d_model, d_discr, bias=False), nn.Linear(d_discr, d_model, bias=False),)\n",
    "        self.conv = nn.Conv1d(\n",
    "            in_channels=d_model,\n",
    "            out_channels=d_model,\n",
    "            kernel_size=ker_size,\n",
    "            padding=ker_size - 1,\n",
    "            groups=d_model,\n",
    "            bias=True,\n",
    "        )\n",
    "        self.A = nn.Parameter(torch.arange(1, d_state + 1, dtype=torch.float).repeat(d_model, 1))\n",
    "        self.D = nn.Parameter(torch.ones(d_model, dtype=torch.float))\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        \n",
    "    def forward(self, seq, cache=None):\n",
    "        b, l, d = seq.shape\n",
    "        (prev_hid, prev_inp) = cache if cache is not None else (None, None)\n",
    "        a, b = self.in_proj(seq).chunk(2, dim=-1)\n",
    "        x = rearrange(a, 'b l d -> b d l')\n",
    "        x = x if prev_inp is None else torch.cat((prev_inp, x), dim=-1)\n",
    "        a = self.conv(x)[..., :l]\n",
    "        a = rearrange(a, 'b d l -> b l d')\n",
    "        a = silu(a)\n",
    "        a, hid = self.ssm(a, prev_hid=prev_hid) \n",
    "        b = silu(b)\n",
    "        out = a * b\n",
    "        out =  self.out_proj(out)\n",
    "        if cache:\n",
    "            cache = (hid.squeeze(), x[..., 1:])   \n",
    "        return out, cache\n",
    "    \n",
    "    def ssm(self, seq, prev_hid):\n",
    "        A = -self.A\n",
    "        D = +self.D\n",
    "        B = self.s_B(seq)\n",
    "        C = self.s_C(seq)\n",
    "        s = softplus(D + self.s_D(seq))\n",
    "        A_bar = einsum(torch.exp(A), s, 'd s,   b l d -> b l d s')\n",
    "        B_bar = einsum(          B,  s, 'b l s, b l d -> b l d s')\n",
    "        X_bar = einsum(B_bar, seq, 'b l d s, b l d -> b l d s')\n",
    "        hid = self._hid_states(A_bar, X_bar, prev_hid=prev_hid)\n",
    "        out = einsum(hid, C, 'b l d s, b l s -> b l d')\n",
    "        out = out + D * seq\n",
    "        return out, hid\n",
    "    \n",
    "    def _hid_states(self, A, X, prev_hid=None):\n",
    "        b, l, d, s = A.shape\n",
    "        A = rearrange(A, 'b l d s -> l b d s')\n",
    "        X = rearrange(X, 'b l d s -> l b d s')\n",
    "        if prev_hid is not None:\n",
    "            return rearrange(A * prev_hid + X, 'l b d s -> b l d s')\n",
    "        h = torch.zeros(b, d, s, device=self.device)\n",
    "        return torch.stack([h := A_t * h + X_t for A_t, X_t in zip(A, X)], dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "419fea30-ace3-4f5f-b234-d3a725c7b1c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2f4f597-8b7f-415f-b52c-dc4215bee9da",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture --no-stdout\n",
    "model = Mamba(num_layers = 2, d_input = 1024, d_model = 512, num_classes=7, model_name='jina', pooling=None).to(device)\n",
    "PATH_TO_MODEL = os.path.join(os.path.join(\".\"), \"models\")\n",
    "checkpoint = torch.load(os.path.join(PATH_TO_MODEL, \"Mamba_jina_checkpoint.pth\"))\n",
    "model.load_state_dict(checkpoint['model_state_dict'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "e492baa6-de85-40db-a020-3872d018eb49",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['neutral', 'surprise/enthusiasm', 'neutral', 'anger', 'sadness', 'disgust']"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict(['hello', 'Wow! This for me??', 'I work as a teacher.', 'Go away, I hate you!', \"I cried all night\", \"It's so dirty here\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "a04747d7-1fb9-42e7-adcd-d05b3cf5ebcb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['anger', 'fear', 'neutral', 'joy/happiness', 'surprise/enthusiasm']"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict(['Уходи отсюда! Ты все испортил!', 'Мне страшно!', 'Я работаю в школе', 'Ура! Спасибо!', \"Вау! Это мне??\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Google Colab Analog 2024 (PyTorch 2.5.1 + TensorFlow 2.18) [python-google_colab_gpu_2024]",
   "language": "python",
   "name": "conda-env-python-google_colab_gpu_2024-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
