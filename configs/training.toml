# ---------------------------
# Общие параметры тренировки
# ---------------------------
[train.general]
random_seed = 42         # фиксируем random seed для воспроизводимости (0 = каждый раз разный)
subset_size = 100         # ограничение на количество примеров (0 = использовать весь датасет)
merge_probability = 0    # процент склеивания коротких файлов
batch_size = 8         # размер батча
num_epochs = 75           # число эпох тренировки
max_patience = 10        # максимальное число эпох без улучшений (для Early Stopping)
save_best_model = false
save_prepared_data = true # сохранять извлеченные признаки (эмбеддинги)
save_feature_path = 'features/' # путь для сохранения эмбеддингов
search_type = "none" # стратегия поиска: "greedy", "exhaustive" или "none"
path_to_df_ls = 'LLM_labels/Phi-4-mini-instruct_emotions_union.csv'  # путь к датафрейму со смягченными метками - Qwen3-4B_emotions_union или Phi-4-mini-instruct_emotions_union
smoothing_probability = 0.0 # процент использования смягченных меток

# ---------------------------
# Параметры модели
# ---------------------------
[train.model]
model_name = "BiFormer"    # название модели (BiGraphFormer, BiFormer, BiGatedGraphFormer, BiGatedFormer, BiMamba, PredictionsFusion, BiFormerWithProb, BiMambaWithProb, BiGraphFormerWithProb, BiGatedGraphFormerWithProb)
hidden_dim = 256            # размер скрытого состояния
hidden_dim_gated = 128      # скрытое состояние для gated механизмов
num_transformer_heads = 16   # количество attention голов в трансформере
num_graph_heads = 2         # количество голов в граф-механизме
tr_layer_number = 5         # количество слоев в трансформере
mamba_d_state = 16          # размер состояния в Mamba
mamba_ker_size = 6          # размер кернела в Mamba
mamba_layer_number = 5      # количество слоев Mamba
positional_encoding = false # использовать ли позиционное кодирование
dropout = 0.15              # dropout между слоями
out_features = 256          # размер финальных признаков перед классификацией
mode = 'mean'               # способ агрегации признаков (например, "mean", "max", и т.д.)

# ---------------------------
# Параметры оптимизатора
# ---------------------------
[train.optimizer]
optimizer = "adam"        # тип оптимизатора: "adam", "adamw", "lion", "sgd", "rmsprop"
lr = 1e-4                 # начальная скорость обучения
weight_decay = 0.0        # weight decay для регуляризации
momentum = 0.9            # momentum (используется только в SGD)

# ---------------------------
# Параметры шедулера
# ---------------------------
[train.scheduler]
scheduler_type = "plateau" # тип шедулера: "none", "plateau", "cosine", "onecycle" ил  и HuggingFace-стиль ("huggingface_linear", "huggingface_cosine" "huggingface_cosine_with_restarts" и т.д.)
warmup_ratio = 0.1         # отношение количества warmup-итераций к общему числу шагов (0.1 = 10%)
